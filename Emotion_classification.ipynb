{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0vB_TkfHwAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXR4wUrHx_C",
        "colab_type": "code",
        "outputId": "48a1bb45-b6bb-4215-fa9b-045a20865e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boQf9s5MbTxB",
        "colab_type": "code",
        "outputId": "2ee11f88-e1d5-474b-b92e-8f9bf3c49c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pip install librosa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.0)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.3)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.30.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIYmKIr9bcBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7svUw4efbk5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from librosa import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBN3KlkKbrIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, sampling_rate =librosa.load('/content/drive/My Drive/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2CJpl8cDfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PNHp2ofsKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7flp6k_3fvQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF_x1qmRgQBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIP9c4FOgTp3",
        "colab_type": "code",
        "outputId": "cc9cae63-b4e3-4b30-8a70-2c907dde6354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "plt.figure(figsize=(12, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-bg3xyfgjzX",
        "colab_type": "code",
        "outputId": "25368a99-ddb7-4092-d331-8c2d9da529c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f7c72752fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd-0gpgJgwi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "path = '/content/drive/My Drive/Ravdess'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvVgTorqhf63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lst = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDuEoggmLXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKBX0DRth-Cj",
        "colab_type": "code",
        "outputId": "35664f46-126d-4e8a-e7d8-738a1b32f96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 1698.0222883224487 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StgE-PO-nxVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRS3TGAFn3mI",
        "colab_type": "code",
        "outputId": "be9fb78a-3157-4ec7-c009-6d1d643348d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2900, 40), (2900,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APylOzN1oPW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving joblib files to not load them again with the loop above\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/My Drive/Ravdess'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqCJcgN6oQ0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading saved models\n",
        "\n",
        "X = joblib.load('/content/drive/My Drive/Ravdess/X.joblib')\n",
        "y = joblib.load('/content/drive/My Drive/Ravdess/y.joblib')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TQfuggHp1eV",
        "colab_type": "text"
      },
      "source": [
        "#Decision Tree classifier\n",
        "#To make a first attempt in accomplishing this classification task I chose a decision tree:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eDQbzuAorfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xse22BxTox69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1o_kHT7o1Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhQi_rDzo5wj",
        "colab_type": "code",
        "outputId": "961900e8-f81d-488d-8aad-ee8f96815232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X1eBTtno9j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96SsSNlpo74",
        "colab_type": "text"
      },
      "source": [
        "Let's go with our classification report.\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "emotions = { \"neutral\": \"0\", \"calm\": \"1\", \"happy\": \"2\", \"sad\": \"3\", \"angry\": \"4\", \"fearful\": \"5\", \"disgust\": \"6\", \"surprised\": \"7\" }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7hu9zQkprFJ",
        "colab_type": "code",
        "outputId": "424881a5-0ce8-41a4-c9bc-f88e7ba581be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.73      0.83        73\n",
            "           1       0.89      0.89      0.89       122\n",
            "           2       0.69      0.67      0.68       121\n",
            "           3       0.74      0.78      0.76       130\n",
            "           4       0.82      0.85      0.84       137\n",
            "           5       0.75      0.85      0.80       120\n",
            "           6       0.88      0.73      0.79       131\n",
            "           7       0.71      0.80      0.76       123\n",
            "\n",
            "    accuracy                           0.79       957\n",
            "   macro avg       0.80      0.79      0.79       957\n",
            "weighted avg       0.80      0.79      0.79       957\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3psLy7XzqKyY",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest¶\n",
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXw-ZCn-qY4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83OrXlY2qwkq",
        "colab_type": "code",
        "outputId": "8db5f30a-38fe-4181-c862-8a0fca38564a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=20,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=22000,\n",
              "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRvP3w7iq4Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzXfZIQlrEFP",
        "colab_type": "code",
        "outputId": "981afc67-f4d4-41d4-e3d3-3e7a9dd29f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.42      0.60        73\n",
            "           1       0.66      0.97      0.78       122\n",
            "           2       0.77      0.69      0.72       121\n",
            "           3       0.77      0.72      0.74       130\n",
            "           4       0.92      0.80      0.85       137\n",
            "           5       0.75      0.79      0.77       120\n",
            "           6       0.77      0.79      0.78       131\n",
            "           7       0.73      0.82      0.77       123\n",
            "\n",
            "    accuracy                           0.77       957\n",
            "   macro avg       0.80      0.75      0.75       957\n",
            "weighted avg       0.79      0.77      0.76       957\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm9meioNrM65",
        "colab_type": "text"
      },
      "source": [
        "# Neural network\n",
        "Let's build our neural network!\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hdDh7jUr6yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbZcGlKgsDg0",
        "colab_type": "code",
        "outputId": "af5349e7-068c-4262-d93c-09b3096b6eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1943, 40, 1), (957, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8rmtUvAwnSD",
        "colab_type": "code",
        "outputId": "9390f386-555e-4349-9549-395430cb4ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceWyMt65sHUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00001,decay=1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyyQvJddsQrW",
        "colab_type": "code",
        "outputId": "2039fbf3-d36f-4d9d-e857-1a2ddf9c2981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXeHdIDZsYs2",
        "colab_type": "code",
        "outputId": "6bf0fb20-5ff2-409c-f68b-a37388dd07c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGajALQWsdOV",
        "colab_type": "code",
        "outputId": "0e5d0520-05f9-47ad-d810-c56f394e7635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1943 samples, validate on 957 samples\n",
            "Epoch 1/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.8960 - acc: 0.6830 - val_loss: 1.0382 - val_acc: 0.6374\n",
            "Epoch 2/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.8954 - acc: 0.6953 - val_loss: 1.0386 - val_acc: 0.6520\n",
            "Epoch 3/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.8895 - acc: 0.6927 - val_loss: 1.0318 - val_acc: 0.6573\n",
            "Epoch 4/1000\n",
            "1943/1943 [==============================] - 1s 490us/step - loss: 0.9031 - acc: 0.6850 - val_loss: 1.0364 - val_acc: 0.6447\n",
            "Epoch 5/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.8966 - acc: 0.6840 - val_loss: 1.0321 - val_acc: 0.6447\n",
            "Epoch 6/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.8910 - acc: 0.7005 - val_loss: 1.0308 - val_acc: 0.6458\n",
            "Epoch 7/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.8756 - acc: 0.7072 - val_loss: 1.0327 - val_acc: 0.6468\n",
            "Epoch 8/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.8928 - acc: 0.7041 - val_loss: 1.0313 - val_acc: 0.6531\n",
            "Epoch 9/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.8947 - acc: 0.6963 - val_loss: 1.0332 - val_acc: 0.6353\n",
            "Epoch 10/1000\n",
            "1943/1943 [==============================] - 1s 464us/step - loss: 0.8706 - acc: 0.7123 - val_loss: 1.0306 - val_acc: 0.6395\n",
            "Epoch 11/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.8968 - acc: 0.6989 - val_loss: 1.0287 - val_acc: 0.6374\n",
            "Epoch 12/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.9040 - acc: 0.6912 - val_loss: 1.0259 - val_acc: 0.6479\n",
            "Epoch 13/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.8788 - acc: 0.7020 - val_loss: 1.0293 - val_acc: 0.6405\n",
            "Epoch 14/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.8839 - acc: 0.7015 - val_loss: 1.0329 - val_acc: 0.6531\n",
            "Epoch 15/1000\n",
            "1943/1943 [==============================] - 1s 505us/step - loss: 0.8775 - acc: 0.6902 - val_loss: 1.0275 - val_acc: 0.6426\n",
            "Epoch 16/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8790 - acc: 0.6979 - val_loss: 1.0306 - val_acc: 0.6468\n",
            "Epoch 17/1000\n",
            "1943/1943 [==============================] - 1s 463us/step - loss: 0.8935 - acc: 0.7005 - val_loss: 1.0337 - val_acc: 0.6437\n",
            "Epoch 18/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8910 - acc: 0.6881 - val_loss: 1.0279 - val_acc: 0.6468\n",
            "Epoch 19/1000\n",
            "1943/1943 [==============================] - 1s 459us/step - loss: 0.8598 - acc: 0.7113 - val_loss: 1.0320 - val_acc: 0.6531\n",
            "Epoch 20/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8834 - acc: 0.7015 - val_loss: 1.0308 - val_acc: 0.6510\n",
            "Epoch 21/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.8799 - acc: 0.7087 - val_loss: 1.0255 - val_acc: 0.6499\n",
            "Epoch 22/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.8753 - acc: 0.6963 - val_loss: 1.0233 - val_acc: 0.6531\n",
            "Epoch 23/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.8733 - acc: 0.7113 - val_loss: 1.0245 - val_acc: 0.6489\n",
            "Epoch 24/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.8777 - acc: 0.7005 - val_loss: 1.0248 - val_acc: 0.6520\n",
            "Epoch 25/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.8835 - acc: 0.6984 - val_loss: 1.0240 - val_acc: 0.6479\n",
            "Epoch 26/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.8815 - acc: 0.7077 - val_loss: 1.0265 - val_acc: 0.6426\n",
            "Epoch 27/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8808 - acc: 0.7066 - val_loss: 1.0253 - val_acc: 0.6416\n",
            "Epoch 28/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.8763 - acc: 0.7020 - val_loss: 1.0224 - val_acc: 0.6552\n",
            "Epoch 29/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8793 - acc: 0.6974 - val_loss: 1.0221 - val_acc: 0.6594\n",
            "Epoch 30/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8647 - acc: 0.6989 - val_loss: 1.0287 - val_acc: 0.6479\n",
            "Epoch 31/1000\n",
            "1943/1943 [==============================] - 1s 460us/step - loss: 0.8848 - acc: 0.7025 - val_loss: 1.0235 - val_acc: 0.6468\n",
            "Epoch 32/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8875 - acc: 0.7010 - val_loss: 1.0264 - val_acc: 0.6395\n",
            "Epoch 33/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.8888 - acc: 0.6943 - val_loss: 1.0221 - val_acc: 0.6426\n",
            "Epoch 34/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.8808 - acc: 0.7056 - val_loss: 1.0235 - val_acc: 0.6405\n",
            "Epoch 35/1000\n",
            "1943/1943 [==============================] - 1s 467us/step - loss: 0.8750 - acc: 0.7066 - val_loss: 1.0265 - val_acc: 0.6552\n",
            "Epoch 36/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8848 - acc: 0.7015 - val_loss: 1.0239 - val_acc: 0.6468\n",
            "Epoch 37/1000\n",
            "1943/1943 [==============================] - 1s 473us/step - loss: 0.8739 - acc: 0.7082 - val_loss: 1.0225 - val_acc: 0.6416\n",
            "Epoch 38/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.8514 - acc: 0.7128 - val_loss: 1.0223 - val_acc: 0.6552\n",
            "Epoch 39/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.8658 - acc: 0.7036 - val_loss: 1.0218 - val_acc: 0.6458\n",
            "Epoch 40/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.8773 - acc: 0.6979 - val_loss: 1.0201 - val_acc: 0.6520\n",
            "Epoch 41/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8741 - acc: 0.7041 - val_loss: 1.0180 - val_acc: 0.6583\n",
            "Epoch 42/1000\n",
            "1943/1943 [==============================] - 1s 481us/step - loss: 0.8712 - acc: 0.7046 - val_loss: 1.0214 - val_acc: 0.6520\n",
            "Epoch 43/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8737 - acc: 0.6979 - val_loss: 1.0214 - val_acc: 0.6520\n",
            "Epoch 44/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.8939 - acc: 0.6866 - val_loss: 1.0165 - val_acc: 0.6552\n",
            "Epoch 45/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8603 - acc: 0.7102 - val_loss: 1.0189 - val_acc: 0.6499\n",
            "Epoch 46/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8652 - acc: 0.7061 - val_loss: 1.0148 - val_acc: 0.6510\n",
            "Epoch 47/1000\n",
            "1943/1943 [==============================] - 1s 459us/step - loss: 0.8769 - acc: 0.7092 - val_loss: 1.0130 - val_acc: 0.6594\n",
            "Epoch 48/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.8765 - acc: 0.7066 - val_loss: 1.0184 - val_acc: 0.6468\n",
            "Epoch 49/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.8606 - acc: 0.7072 - val_loss: 1.0217 - val_acc: 0.6562\n",
            "Epoch 50/1000\n",
            "1943/1943 [==============================] - 1s 450us/step - loss: 0.8506 - acc: 0.7082 - val_loss: 1.0167 - val_acc: 0.6499\n",
            "Epoch 51/1000\n",
            "1943/1943 [==============================] - 1s 462us/step - loss: 0.8589 - acc: 0.7005 - val_loss: 1.0175 - val_acc: 0.6604\n",
            "Epoch 52/1000\n",
            "1943/1943 [==============================] - 1s 456us/step - loss: 0.8627 - acc: 0.7020 - val_loss: 1.0155 - val_acc: 0.6531\n",
            "Epoch 53/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8677 - acc: 0.6958 - val_loss: 1.0172 - val_acc: 0.6520\n",
            "Epoch 54/1000\n",
            "1943/1943 [==============================] - 1s 462us/step - loss: 0.8584 - acc: 0.7056 - val_loss: 1.0212 - val_acc: 0.6447\n",
            "Epoch 55/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8562 - acc: 0.7010 - val_loss: 1.0181 - val_acc: 0.6468\n",
            "Epoch 56/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.8808 - acc: 0.7025 - val_loss: 1.0117 - val_acc: 0.6489\n",
            "Epoch 57/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8693 - acc: 0.7118 - val_loss: 1.0120 - val_acc: 0.6479\n",
            "Epoch 58/1000\n",
            "1943/1943 [==============================] - 1s 462us/step - loss: 0.8682 - acc: 0.7087 - val_loss: 1.0152 - val_acc: 0.6541\n",
            "Epoch 59/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8571 - acc: 0.7133 - val_loss: 1.0079 - val_acc: 0.6625\n",
            "Epoch 60/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.8690 - acc: 0.7010 - val_loss: 1.0108 - val_acc: 0.6510\n",
            "Epoch 61/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.8533 - acc: 0.6999 - val_loss: 1.0143 - val_acc: 0.6625\n",
            "Epoch 62/1000\n",
            "1943/1943 [==============================] - 1s 466us/step - loss: 0.8499 - acc: 0.7087 - val_loss: 1.0130 - val_acc: 0.6395\n",
            "Epoch 63/1000\n",
            "1943/1943 [==============================] - 1s 494us/step - loss: 0.8603 - acc: 0.7030 - val_loss: 1.0067 - val_acc: 0.6552\n",
            "Epoch 64/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8540 - acc: 0.7030 - val_loss: 1.0098 - val_acc: 0.6395\n",
            "Epoch 65/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8653 - acc: 0.7046 - val_loss: 1.0135 - val_acc: 0.6531\n",
            "Epoch 66/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8525 - acc: 0.7097 - val_loss: 1.0088 - val_acc: 0.6520\n",
            "Epoch 67/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.8626 - acc: 0.7149 - val_loss: 1.0140 - val_acc: 0.6416\n",
            "Epoch 68/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.8534 - acc: 0.6958 - val_loss: 1.0107 - val_acc: 0.6489\n",
            "Epoch 69/1000\n",
            "1943/1943 [==============================] - 1s 467us/step - loss: 0.8423 - acc: 0.7169 - val_loss: 1.0162 - val_acc: 0.6562\n",
            "Epoch 70/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8610 - acc: 0.7005 - val_loss: 1.0074 - val_acc: 0.6552\n",
            "Epoch 71/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.8353 - acc: 0.7185 - val_loss: 1.0035 - val_acc: 0.6499\n",
            "Epoch 72/1000\n",
            "1943/1943 [==============================] - 1s 458us/step - loss: 0.8578 - acc: 0.7205 - val_loss: 1.0058 - val_acc: 0.6531\n",
            "Epoch 73/1000\n",
            "1943/1943 [==============================] - 1s 473us/step - loss: 0.8625 - acc: 0.7041 - val_loss: 1.0029 - val_acc: 0.6489\n",
            "Epoch 74/1000\n",
            "1943/1943 [==============================] - 1s 464us/step - loss: 0.8436 - acc: 0.7113 - val_loss: 1.0068 - val_acc: 0.6541\n",
            "Epoch 75/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.8475 - acc: 0.7200 - val_loss: 1.0019 - val_acc: 0.6604\n",
            "Epoch 76/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8566 - acc: 0.7046 - val_loss: 1.0081 - val_acc: 0.6562\n",
            "Epoch 77/1000\n",
            "1943/1943 [==============================] - 1s 493us/step - loss: 0.8603 - acc: 0.7077 - val_loss: 1.0052 - val_acc: 0.6646\n",
            "Epoch 78/1000\n",
            "1943/1943 [==============================] - 1s 499us/step - loss: 0.8585 - acc: 0.7005 - val_loss: 1.0046 - val_acc: 0.6614\n",
            "Epoch 79/1000\n",
            "1943/1943 [==============================] - 1s 493us/step - loss: 0.8450 - acc: 0.7267 - val_loss: 1.0011 - val_acc: 0.6531\n",
            "Epoch 80/1000\n",
            "1943/1943 [==============================] - 1s 500us/step - loss: 0.8440 - acc: 0.7159 - val_loss: 1.0031 - val_acc: 0.6614\n",
            "Epoch 81/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8455 - acc: 0.7205 - val_loss: 1.0089 - val_acc: 0.6552\n",
            "Epoch 82/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8434 - acc: 0.7195 - val_loss: 0.9998 - val_acc: 0.6541\n",
            "Epoch 83/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.8444 - acc: 0.7190 - val_loss: 1.0029 - val_acc: 0.6552\n",
            "Epoch 84/1000\n",
            "1943/1943 [==============================] - 1s 467us/step - loss: 0.8325 - acc: 0.7236 - val_loss: 1.0027 - val_acc: 0.6583\n",
            "Epoch 85/1000\n",
            "1943/1943 [==============================] - 1s 512us/step - loss: 0.8543 - acc: 0.7097 - val_loss: 1.0034 - val_acc: 0.6698\n",
            "Epoch 86/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.8340 - acc: 0.7174 - val_loss: 0.9968 - val_acc: 0.6531\n",
            "Epoch 87/1000\n",
            "1943/1943 [==============================] - 1s 575us/step - loss: 0.8269 - acc: 0.7226 - val_loss: 1.0023 - val_acc: 0.6688\n",
            "Epoch 88/1000\n",
            "1943/1943 [==============================] - 1s 603us/step - loss: 0.8429 - acc: 0.7164 - val_loss: 0.9970 - val_acc: 0.6562\n",
            "Epoch 89/1000\n",
            "1943/1943 [==============================] - 1s 505us/step - loss: 0.8477 - acc: 0.7128 - val_loss: 1.0002 - val_acc: 0.6573\n",
            "Epoch 90/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.8478 - acc: 0.6999 - val_loss: 0.9996 - val_acc: 0.6635\n",
            "Epoch 91/1000\n",
            "1943/1943 [==============================] - 1s 572us/step - loss: 0.8459 - acc: 0.7195 - val_loss: 1.0028 - val_acc: 0.6646\n",
            "Epoch 92/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.8531 - acc: 0.7005 - val_loss: 0.9985 - val_acc: 0.6625\n",
            "Epoch 93/1000\n",
            "1943/1943 [==============================] - 1s 515us/step - loss: 0.8576 - acc: 0.7210 - val_loss: 0.9976 - val_acc: 0.6552\n",
            "Epoch 94/1000\n",
            "1943/1943 [==============================] - 1s 501us/step - loss: 0.8351 - acc: 0.7180 - val_loss: 0.9957 - val_acc: 0.6499\n",
            "Epoch 95/1000\n",
            "1943/1943 [==============================] - 1s 570us/step - loss: 0.8460 - acc: 0.7247 - val_loss: 1.0002 - val_acc: 0.6531\n",
            "Epoch 96/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.8320 - acc: 0.7303 - val_loss: 0.9960 - val_acc: 0.6594\n",
            "Epoch 97/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.8506 - acc: 0.7082 - val_loss: 0.9986 - val_acc: 0.6520\n",
            "Epoch 98/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.8309 - acc: 0.7128 - val_loss: 0.9990 - val_acc: 0.6698\n",
            "Epoch 99/1000\n",
            "1943/1943 [==============================] - 1s 462us/step - loss: 0.8390 - acc: 0.7174 - val_loss: 0.9965 - val_acc: 0.6635\n",
            "Epoch 100/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8392 - acc: 0.7061 - val_loss: 0.9925 - val_acc: 0.6614\n",
            "Epoch 101/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8424 - acc: 0.7123 - val_loss: 0.9958 - val_acc: 0.6625\n",
            "Epoch 102/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8401 - acc: 0.6979 - val_loss: 0.9926 - val_acc: 0.6698\n",
            "Epoch 103/1000\n",
            "1943/1943 [==============================] - 1s 495us/step - loss: 0.8342 - acc: 0.7174 - val_loss: 0.9895 - val_acc: 0.6583\n",
            "Epoch 104/1000\n",
            "1943/1943 [==============================] - 1s 505us/step - loss: 0.8318 - acc: 0.7164 - val_loss: 0.9944 - val_acc: 0.6604\n",
            "Epoch 105/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.8444 - acc: 0.7092 - val_loss: 0.9977 - val_acc: 0.6583\n",
            "Epoch 106/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.8346 - acc: 0.7174 - val_loss: 0.9891 - val_acc: 0.6635\n",
            "Epoch 107/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.8370 - acc: 0.7133 - val_loss: 0.9897 - val_acc: 0.6708\n",
            "Epoch 108/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.8308 - acc: 0.7231 - val_loss: 0.9861 - val_acc: 0.6625\n",
            "Epoch 109/1000\n",
            "1943/1943 [==============================] - 1s 489us/step - loss: 0.8157 - acc: 0.7313 - val_loss: 0.9932 - val_acc: 0.6614\n",
            "Epoch 110/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.8259 - acc: 0.7226 - val_loss: 0.9904 - val_acc: 0.6625\n",
            "Epoch 111/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.8253 - acc: 0.7159 - val_loss: 0.9915 - val_acc: 0.6677\n",
            "Epoch 112/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.8415 - acc: 0.7092 - val_loss: 0.9924 - val_acc: 0.6698\n",
            "Epoch 113/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.8369 - acc: 0.7087 - val_loss: 0.9882 - val_acc: 0.6646\n",
            "Epoch 114/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.8362 - acc: 0.7077 - val_loss: 0.9938 - val_acc: 0.6510\n",
            "Epoch 115/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.8360 - acc: 0.7133 - val_loss: 0.9867 - val_acc: 0.6479\n",
            "Epoch 116/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.8374 - acc: 0.7066 - val_loss: 0.9876 - val_acc: 0.6479\n",
            "Epoch 117/1000\n",
            "1943/1943 [==============================] - 1s 504us/step - loss: 0.8396 - acc: 0.7123 - val_loss: 0.9898 - val_acc: 0.6698\n",
            "Epoch 118/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8307 - acc: 0.7138 - val_loss: 0.9872 - val_acc: 0.6552\n",
            "Epoch 119/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.8328 - acc: 0.7149 - val_loss: 0.9838 - val_acc: 0.6708\n",
            "Epoch 120/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.8179 - acc: 0.7257 - val_loss: 0.9854 - val_acc: 0.6499\n",
            "Epoch 121/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.8268 - acc: 0.7169 - val_loss: 0.9860 - val_acc: 0.6667\n",
            "Epoch 122/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.8259 - acc: 0.7154 - val_loss: 0.9846 - val_acc: 0.6635\n",
            "Epoch 123/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.8246 - acc: 0.7257 - val_loss: 0.9889 - val_acc: 0.6625\n",
            "Epoch 124/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.8274 - acc: 0.7123 - val_loss: 0.9876 - val_acc: 0.6719\n",
            "Epoch 125/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.8314 - acc: 0.7200 - val_loss: 0.9820 - val_acc: 0.6510\n",
            "Epoch 126/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.8336 - acc: 0.7257 - val_loss: 0.9824 - val_acc: 0.6646\n",
            "Epoch 127/1000\n",
            "1943/1943 [==============================] - 1s 499us/step - loss: 0.8340 - acc: 0.7149 - val_loss: 0.9879 - val_acc: 0.6719\n",
            "Epoch 128/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.8300 - acc: 0.7195 - val_loss: 0.9845 - val_acc: 0.6614\n",
            "Epoch 129/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.7962 - acc: 0.7355 - val_loss: 0.9841 - val_acc: 0.6614\n",
            "Epoch 130/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.8237 - acc: 0.7226 - val_loss: 0.9825 - val_acc: 0.6635\n",
            "Epoch 131/1000\n",
            "1943/1943 [==============================] - 1s 496us/step - loss: 0.8213 - acc: 0.7236 - val_loss: 0.9852 - val_acc: 0.6688\n",
            "Epoch 132/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.8190 - acc: 0.7241 - val_loss: 0.9847 - val_acc: 0.6771\n",
            "Epoch 133/1000\n",
            "1943/1943 [==============================] - 1s 502us/step - loss: 0.8298 - acc: 0.7210 - val_loss: 0.9824 - val_acc: 0.6594\n",
            "Epoch 134/1000\n",
            "1943/1943 [==============================] - 1s 512us/step - loss: 0.8143 - acc: 0.7154 - val_loss: 0.9819 - val_acc: 0.6688\n",
            "Epoch 135/1000\n",
            "1943/1943 [==============================] - 1s 506us/step - loss: 0.8063 - acc: 0.7277 - val_loss: 0.9792 - val_acc: 0.6625\n",
            "Epoch 136/1000\n",
            "1943/1943 [==============================] - 1s 491us/step - loss: 0.8188 - acc: 0.7355 - val_loss: 0.9792 - val_acc: 0.6667\n",
            "Epoch 137/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.8362 - acc: 0.7082 - val_loss: 0.9849 - val_acc: 0.6698\n",
            "Epoch 138/1000\n",
            "1943/1943 [==============================] - 1s 504us/step - loss: 0.8128 - acc: 0.7262 - val_loss: 0.9815 - val_acc: 0.6677\n",
            "Epoch 139/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.8150 - acc: 0.7288 - val_loss: 0.9813 - val_acc: 0.6698\n",
            "Epoch 140/1000\n",
            "1943/1943 [==============================] - 1s 473us/step - loss: 0.8348 - acc: 0.7159 - val_loss: 0.9808 - val_acc: 0.6604\n",
            "Epoch 141/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.8245 - acc: 0.7210 - val_loss: 0.9821 - val_acc: 0.6604\n",
            "Epoch 142/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.8262 - acc: 0.7149 - val_loss: 0.9864 - val_acc: 0.6656\n",
            "Epoch 143/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.8109 - acc: 0.7262 - val_loss: 0.9766 - val_acc: 0.6656\n",
            "Epoch 144/1000\n",
            "1943/1943 [==============================] - 1s 467us/step - loss: 0.8115 - acc: 0.7205 - val_loss: 0.9808 - val_acc: 0.6656\n",
            "Epoch 145/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.8252 - acc: 0.7087 - val_loss: 0.9784 - val_acc: 0.6698\n",
            "Epoch 146/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.8247 - acc: 0.7097 - val_loss: 0.9827 - val_acc: 0.6594\n",
            "Epoch 147/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.8087 - acc: 0.7174 - val_loss: 0.9774 - val_acc: 0.6677\n",
            "Epoch 148/1000\n",
            "1943/1943 [==============================] - 1s 490us/step - loss: 0.8056 - acc: 0.7185 - val_loss: 0.9806 - val_acc: 0.6635\n",
            "Epoch 149/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.8039 - acc: 0.7334 - val_loss: 0.9814 - val_acc: 0.6594\n",
            "Epoch 150/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8138 - acc: 0.7308 - val_loss: 0.9778 - val_acc: 0.6646\n",
            "Epoch 151/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7979 - acc: 0.7401 - val_loss: 0.9793 - val_acc: 0.6656\n",
            "Epoch 152/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.8102 - acc: 0.7226 - val_loss: 0.9746 - val_acc: 0.6729\n",
            "Epoch 153/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.8131 - acc: 0.7216 - val_loss: 0.9812 - val_acc: 0.6635\n",
            "Epoch 154/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.8136 - acc: 0.7293 - val_loss: 0.9763 - val_acc: 0.6667\n",
            "Epoch 155/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.8168 - acc: 0.7257 - val_loss: 0.9744 - val_acc: 0.6750\n",
            "Epoch 156/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.8100 - acc: 0.7236 - val_loss: 0.9738 - val_acc: 0.6740\n",
            "Epoch 157/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.8229 - acc: 0.7236 - val_loss: 0.9745 - val_acc: 0.6698\n",
            "Epoch 158/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.8147 - acc: 0.7210 - val_loss: 0.9728 - val_acc: 0.6594\n",
            "Epoch 159/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.7926 - acc: 0.7504 - val_loss: 0.9687 - val_acc: 0.6719\n",
            "Epoch 160/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.8054 - acc: 0.7324 - val_loss: 0.9681 - val_acc: 0.6708\n",
            "Epoch 161/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.8021 - acc: 0.7344 - val_loss: 0.9708 - val_acc: 0.6719\n",
            "Epoch 162/1000\n",
            "1943/1943 [==============================] - 1s 510us/step - loss: 0.8156 - acc: 0.7288 - val_loss: 0.9740 - val_acc: 0.6646\n",
            "Epoch 163/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.7909 - acc: 0.7349 - val_loss: 0.9748 - val_acc: 0.6792\n",
            "Epoch 164/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.8055 - acc: 0.7288 - val_loss: 0.9679 - val_acc: 0.6677\n",
            "Epoch 165/1000\n",
            "1943/1943 [==============================] - 1s 497us/step - loss: 0.8020 - acc: 0.7313 - val_loss: 0.9691 - val_acc: 0.6677\n",
            "Epoch 166/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.7969 - acc: 0.7339 - val_loss: 0.9716 - val_acc: 0.6729\n",
            "Epoch 167/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.8060 - acc: 0.7319 - val_loss: 0.9670 - val_acc: 0.6677\n",
            "Epoch 168/1000\n",
            "1943/1943 [==============================] - 1s 495us/step - loss: 0.8107 - acc: 0.7236 - val_loss: 0.9676 - val_acc: 0.6761\n",
            "Epoch 169/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.8081 - acc: 0.7401 - val_loss: 0.9664 - val_acc: 0.6740\n",
            "Epoch 170/1000\n",
            "1943/1943 [==============================] - 1s 500us/step - loss: 0.8032 - acc: 0.7288 - val_loss: 0.9649 - val_acc: 0.6646\n",
            "Epoch 171/1000\n",
            "1943/1943 [==============================] - 1s 510us/step - loss: 0.8055 - acc: 0.7349 - val_loss: 0.9695 - val_acc: 0.6625\n",
            "Epoch 172/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.7981 - acc: 0.7334 - val_loss: 0.9828 - val_acc: 0.6614\n",
            "Epoch 173/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.8022 - acc: 0.7169 - val_loss: 0.9688 - val_acc: 0.6594\n",
            "Epoch 174/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.7985 - acc: 0.7313 - val_loss: 0.9756 - val_acc: 0.6646\n",
            "Epoch 175/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.8048 - acc: 0.7355 - val_loss: 0.9686 - val_acc: 0.6761\n",
            "Epoch 176/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.7914 - acc: 0.7277 - val_loss: 0.9730 - val_acc: 0.6761\n",
            "Epoch 177/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.8057 - acc: 0.7334 - val_loss: 0.9664 - val_acc: 0.6614\n",
            "Epoch 178/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.8054 - acc: 0.7205 - val_loss: 0.9676 - val_acc: 0.6708\n",
            "Epoch 179/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.7969 - acc: 0.7406 - val_loss: 0.9685 - val_acc: 0.6573\n",
            "Epoch 180/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.7959 - acc: 0.7313 - val_loss: 0.9645 - val_acc: 0.6646\n",
            "Epoch 181/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.7889 - acc: 0.7416 - val_loss: 0.9653 - val_acc: 0.6719\n",
            "Epoch 182/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.7976 - acc: 0.7262 - val_loss: 0.9651 - val_acc: 0.6740\n",
            "Epoch 183/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.8146 - acc: 0.7149 - val_loss: 0.9649 - val_acc: 0.6782\n",
            "Epoch 184/1000\n",
            "1943/1943 [==============================] - 1s 459us/step - loss: 0.7848 - acc: 0.7401 - val_loss: 0.9584 - val_acc: 0.6708\n",
            "Epoch 185/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.7895 - acc: 0.7298 - val_loss: 0.9626 - val_acc: 0.6813\n",
            "Epoch 186/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.7945 - acc: 0.7422 - val_loss: 0.9605 - val_acc: 0.6688\n",
            "Epoch 187/1000\n",
            "1943/1943 [==============================] - 1s 493us/step - loss: 0.7900 - acc: 0.7355 - val_loss: 0.9578 - val_acc: 0.6740\n",
            "Epoch 188/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.7929 - acc: 0.7252 - val_loss: 0.9587 - val_acc: 0.6708\n",
            "Epoch 189/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.7972 - acc: 0.7313 - val_loss: 0.9583 - val_acc: 0.6594\n",
            "Epoch 190/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.7823 - acc: 0.7401 - val_loss: 0.9594 - val_acc: 0.6552\n",
            "Epoch 191/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.7888 - acc: 0.7344 - val_loss: 0.9636 - val_acc: 0.6740\n",
            "Epoch 192/1000\n",
            "1943/1943 [==============================] - 1s 458us/step - loss: 0.7886 - acc: 0.7277 - val_loss: 0.9609 - val_acc: 0.6740\n",
            "Epoch 193/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.7879 - acc: 0.7339 - val_loss: 0.9597 - val_acc: 0.6792\n",
            "Epoch 194/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.7822 - acc: 0.7308 - val_loss: 0.9596 - val_acc: 0.6708\n",
            "Epoch 195/1000\n",
            "1943/1943 [==============================] - 1s 466us/step - loss: 0.7912 - acc: 0.7313 - val_loss: 0.9597 - val_acc: 0.6761\n",
            "Epoch 196/1000\n",
            "1943/1943 [==============================] - 1s 462us/step - loss: 0.7941 - acc: 0.7231 - val_loss: 0.9598 - val_acc: 0.6761\n",
            "Epoch 197/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.7952 - acc: 0.7308 - val_loss: 0.9565 - val_acc: 0.6834\n",
            "Epoch 198/1000\n",
            "1943/1943 [==============================] - 1s 487us/step - loss: 0.7873 - acc: 0.7401 - val_loss: 0.9593 - val_acc: 0.6750\n",
            "Epoch 199/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.7987 - acc: 0.7411 - val_loss: 0.9535 - val_acc: 0.6740\n",
            "Epoch 200/1000\n",
            "1943/1943 [==============================] - 1s 458us/step - loss: 0.7969 - acc: 0.7339 - val_loss: 0.9539 - val_acc: 0.6813\n",
            "Epoch 201/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.8018 - acc: 0.7365 - val_loss: 0.9647 - val_acc: 0.6886\n",
            "Epoch 202/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.8176 - acc: 0.7185 - val_loss: 0.9584 - val_acc: 0.6625\n",
            "Epoch 203/1000\n",
            "1943/1943 [==============================] - 1s 466us/step - loss: 0.7973 - acc: 0.7313 - val_loss: 0.9626 - val_acc: 0.6708\n",
            "Epoch 204/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.7887 - acc: 0.7396 - val_loss: 0.9555 - val_acc: 0.6677\n",
            "Epoch 205/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.7968 - acc: 0.7344 - val_loss: 0.9605 - val_acc: 0.6782\n",
            "Epoch 206/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.7879 - acc: 0.7370 - val_loss: 0.9564 - val_acc: 0.6803\n",
            "Epoch 207/1000\n",
            "1943/1943 [==============================] - 1s 487us/step - loss: 0.7943 - acc: 0.7324 - val_loss: 0.9602 - val_acc: 0.6761\n",
            "Epoch 208/1000\n",
            "1943/1943 [==============================] - 1s 493us/step - loss: 0.7638 - acc: 0.7416 - val_loss: 0.9527 - val_acc: 0.6813\n",
            "Epoch 209/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.7787 - acc: 0.7380 - val_loss: 0.9495 - val_acc: 0.6771\n",
            "Epoch 210/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.7820 - acc: 0.7406 - val_loss: 0.9495 - val_acc: 0.6719\n",
            "Epoch 211/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.7809 - acc: 0.7267 - val_loss: 0.9508 - val_acc: 0.6688\n",
            "Epoch 212/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.7711 - acc: 0.7494 - val_loss: 0.9553 - val_acc: 0.6677\n",
            "Epoch 213/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.7687 - acc: 0.7447 - val_loss: 0.9562 - val_acc: 0.6708\n",
            "Epoch 214/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.7862 - acc: 0.7401 - val_loss: 0.9517 - val_acc: 0.6823\n",
            "Epoch 215/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.7792 - acc: 0.7360 - val_loss: 0.9535 - val_acc: 0.6740\n",
            "Epoch 216/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.7727 - acc: 0.7319 - val_loss: 0.9546 - val_acc: 0.6688\n",
            "Epoch 217/1000\n",
            "1943/1943 [==============================] - 1s 460us/step - loss: 0.7974 - acc: 0.7241 - val_loss: 0.9483 - val_acc: 0.6886\n",
            "Epoch 218/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7786 - acc: 0.7473 - val_loss: 0.9467 - val_acc: 0.6688\n",
            "Epoch 219/1000\n",
            "1943/1943 [==============================] - 1s 512us/step - loss: 0.7925 - acc: 0.7288 - val_loss: 0.9491 - val_acc: 0.6708\n",
            "Epoch 220/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.7800 - acc: 0.7375 - val_loss: 0.9462 - val_acc: 0.6761\n",
            "Epoch 221/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.7648 - acc: 0.7385 - val_loss: 0.9507 - val_acc: 0.6771\n",
            "Epoch 222/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.7862 - acc: 0.7355 - val_loss: 0.9478 - val_acc: 0.6740\n",
            "Epoch 223/1000\n",
            "1943/1943 [==============================] - 1s 482us/step - loss: 0.7762 - acc: 0.7396 - val_loss: 0.9488 - val_acc: 0.6677\n",
            "Epoch 224/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.7731 - acc: 0.7334 - val_loss: 0.9521 - val_acc: 0.6708\n",
            "Epoch 225/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.7642 - acc: 0.7329 - val_loss: 0.9487 - val_acc: 0.6876\n",
            "Epoch 226/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.7756 - acc: 0.7334 - val_loss: 0.9519 - val_acc: 0.6855\n",
            "Epoch 227/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.7673 - acc: 0.7396 - val_loss: 0.9453 - val_acc: 0.6688\n",
            "Epoch 228/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.7992 - acc: 0.7267 - val_loss: 0.9430 - val_acc: 0.6719\n",
            "Epoch 229/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.7776 - acc: 0.7360 - val_loss: 0.9447 - val_acc: 0.6750\n",
            "Epoch 230/1000\n",
            "1943/1943 [==============================] - 1s 500us/step - loss: 0.7641 - acc: 0.7483 - val_loss: 0.9416 - val_acc: 0.6667\n",
            "Epoch 231/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.7647 - acc: 0.7385 - val_loss: 0.9495 - val_acc: 0.6750\n",
            "Epoch 232/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.7578 - acc: 0.7468 - val_loss: 0.9501 - val_acc: 0.6729\n",
            "Epoch 233/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.7639 - acc: 0.7324 - val_loss: 0.9435 - val_acc: 0.6792\n",
            "Epoch 234/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.7851 - acc: 0.7360 - val_loss: 0.9482 - val_acc: 0.6855\n",
            "Epoch 235/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.7744 - acc: 0.7344 - val_loss: 0.9474 - val_acc: 0.6782\n",
            "Epoch 236/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.7728 - acc: 0.7319 - val_loss: 0.9459 - val_acc: 0.6823\n",
            "Epoch 237/1000\n",
            "1943/1943 [==============================] - 1s 460us/step - loss: 0.7541 - acc: 0.7473 - val_loss: 0.9397 - val_acc: 0.6803\n",
            "Epoch 238/1000\n",
            "1943/1943 [==============================] - 1s 450us/step - loss: 0.7574 - acc: 0.7514 - val_loss: 0.9424 - val_acc: 0.6823\n",
            "Epoch 239/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.7644 - acc: 0.7396 - val_loss: 0.9413 - val_acc: 0.6855\n",
            "Epoch 240/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.7817 - acc: 0.7483 - val_loss: 0.9431 - val_acc: 0.6761\n",
            "Epoch 241/1000\n",
            "1943/1943 [==============================] - 1s 515us/step - loss: 0.7662 - acc: 0.7427 - val_loss: 0.9485 - val_acc: 0.6855\n",
            "Epoch 242/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.7815 - acc: 0.7288 - val_loss: 0.9492 - val_acc: 0.6761\n",
            "Epoch 243/1000\n",
            "1943/1943 [==============================] - 1s 482us/step - loss: 0.7784 - acc: 0.7406 - val_loss: 0.9459 - val_acc: 0.6865\n",
            "Epoch 244/1000\n",
            "1943/1943 [==============================] - 1s 509us/step - loss: 0.7491 - acc: 0.7514 - val_loss: 0.9372 - val_acc: 0.6771\n",
            "Epoch 245/1000\n",
            "1943/1943 [==============================] - 1s 473us/step - loss: 0.7657 - acc: 0.7416 - val_loss: 0.9403 - val_acc: 0.6876\n",
            "Epoch 246/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7436 - acc: 0.7560 - val_loss: 0.9395 - val_acc: 0.6823\n",
            "Epoch 247/1000\n",
            "1943/1943 [==============================] - 1s 466us/step - loss: 0.7635 - acc: 0.7416 - val_loss: 0.9387 - val_acc: 0.6876\n",
            "Epoch 248/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.7660 - acc: 0.7473 - val_loss: 0.9389 - val_acc: 0.6698\n",
            "Epoch 249/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.7698 - acc: 0.7385 - val_loss: 0.9419 - val_acc: 0.6823\n",
            "Epoch 250/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.7698 - acc: 0.7375 - val_loss: 0.9394 - val_acc: 0.6698\n",
            "Epoch 251/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.7643 - acc: 0.7494 - val_loss: 0.9375 - val_acc: 0.6844\n",
            "Epoch 252/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7592 - acc: 0.7458 - val_loss: 0.9396 - val_acc: 0.6698\n",
            "Epoch 253/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.7626 - acc: 0.7396 - val_loss: 0.9408 - val_acc: 0.6917\n",
            "Epoch 254/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.7591 - acc: 0.7463 - val_loss: 0.9410 - val_acc: 0.6740\n",
            "Epoch 255/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.7509 - acc: 0.7535 - val_loss: 0.9419 - val_acc: 0.6876\n",
            "Epoch 256/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.7454 - acc: 0.7524 - val_loss: 0.9389 - val_acc: 0.6729\n",
            "Epoch 257/1000\n",
            "1943/1943 [==============================] - 1s 497us/step - loss: 0.7488 - acc: 0.7504 - val_loss: 0.9435 - val_acc: 0.6917\n",
            "Epoch 258/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.7612 - acc: 0.7514 - val_loss: 0.9385 - val_acc: 0.6928\n",
            "Epoch 259/1000\n",
            "1943/1943 [==============================] - 1s 491us/step - loss: 0.7534 - acc: 0.7411 - val_loss: 0.9413 - val_acc: 0.6876\n",
            "Epoch 260/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.7646 - acc: 0.7422 - val_loss: 0.9465 - val_acc: 0.6886\n",
            "Epoch 261/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.7553 - acc: 0.7406 - val_loss: 0.9336 - val_acc: 0.6886\n",
            "Epoch 262/1000\n",
            "1943/1943 [==============================] - 1s 499us/step - loss: 0.7638 - acc: 0.7483 - val_loss: 0.9397 - val_acc: 0.6897\n",
            "Epoch 263/1000\n",
            "1943/1943 [==============================] - 1s 487us/step - loss: 0.7667 - acc: 0.7344 - val_loss: 0.9393 - val_acc: 0.6729\n",
            "Epoch 264/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.7248 - acc: 0.7607 - val_loss: 0.9345 - val_acc: 0.6677\n",
            "Epoch 265/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.7665 - acc: 0.7483 - val_loss: 0.9403 - val_acc: 0.6823\n",
            "Epoch 266/1000\n",
            "1943/1943 [==============================] - 1s 463us/step - loss: 0.7632 - acc: 0.7406 - val_loss: 0.9373 - val_acc: 0.6803\n",
            "Epoch 267/1000\n",
            "1943/1943 [==============================] - 1s 466us/step - loss: 0.7510 - acc: 0.7391 - val_loss: 0.9308 - val_acc: 0.6813\n",
            "Epoch 268/1000\n",
            "1943/1943 [==============================] - 1s 457us/step - loss: 0.7416 - acc: 0.7494 - val_loss: 0.9338 - val_acc: 0.6886\n",
            "Epoch 269/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7490 - acc: 0.7524 - val_loss: 0.9385 - val_acc: 0.6834\n",
            "Epoch 270/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.7606 - acc: 0.7437 - val_loss: 0.9328 - val_acc: 0.6667\n",
            "Epoch 271/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.7536 - acc: 0.7442 - val_loss: 0.9305 - val_acc: 0.6928\n",
            "Epoch 272/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.7652 - acc: 0.7519 - val_loss: 0.9371 - val_acc: 0.6970\n",
            "Epoch 273/1000\n",
            "1943/1943 [==============================] - 1s 509us/step - loss: 0.7356 - acc: 0.7458 - val_loss: 0.9291 - val_acc: 0.6865\n",
            "Epoch 274/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.7493 - acc: 0.7550 - val_loss: 0.9335 - val_acc: 0.6844\n",
            "Epoch 275/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.7565 - acc: 0.7375 - val_loss: 0.9379 - val_acc: 0.6886\n",
            "Epoch 276/1000\n",
            "1943/1943 [==============================] - 1s 465us/step - loss: 0.7509 - acc: 0.7452 - val_loss: 0.9354 - val_acc: 0.6907\n",
            "Epoch 277/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.7528 - acc: 0.7514 - val_loss: 0.9345 - val_acc: 0.6897\n",
            "Epoch 278/1000\n",
            "1943/1943 [==============================] - 1s 471us/step - loss: 0.7582 - acc: 0.7473 - val_loss: 0.9325 - val_acc: 0.6928\n",
            "Epoch 279/1000\n",
            "1943/1943 [==============================] - 1s 474us/step - loss: 0.7600 - acc: 0.7488 - val_loss: 0.9301 - val_acc: 0.6917\n",
            "Epoch 280/1000\n",
            "1943/1943 [==============================] - 1s 464us/step - loss: 0.7547 - acc: 0.7468 - val_loss: 0.9307 - val_acc: 0.6917\n",
            "Epoch 281/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.7419 - acc: 0.7545 - val_loss: 0.9255 - val_acc: 0.6886\n",
            "Epoch 282/1000\n",
            "1943/1943 [==============================] - 1s 500us/step - loss: 0.7390 - acc: 0.7576 - val_loss: 0.9279 - val_acc: 0.6938\n",
            "Epoch 283/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.7399 - acc: 0.7550 - val_loss: 0.9237 - val_acc: 0.6803\n",
            "Epoch 284/1000\n",
            "1943/1943 [==============================] - 1s 482us/step - loss: 0.7358 - acc: 0.7581 - val_loss: 0.9217 - val_acc: 0.6782\n",
            "Epoch 285/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.7482 - acc: 0.7416 - val_loss: 0.9230 - val_acc: 0.6823\n",
            "Epoch 286/1000\n",
            "1943/1943 [==============================] - 1s 512us/step - loss: 0.7650 - acc: 0.7375 - val_loss: 0.9235 - val_acc: 0.6917\n",
            "Epoch 287/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.7709 - acc: 0.7313 - val_loss: 0.9234 - val_acc: 0.6959\n",
            "Epoch 288/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.7495 - acc: 0.7519 - val_loss: 0.9303 - val_acc: 0.6959\n",
            "Epoch 289/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.7541 - acc: 0.7473 - val_loss: 0.9242 - val_acc: 0.6834\n",
            "Epoch 290/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.7310 - acc: 0.7519 - val_loss: 0.9261 - val_acc: 0.6938\n",
            "Epoch 291/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.7346 - acc: 0.7560 - val_loss: 0.9254 - val_acc: 0.6886\n",
            "Epoch 292/1000\n",
            "1943/1943 [==============================] - 1s 510us/step - loss: 0.7435 - acc: 0.7571 - val_loss: 0.9268 - val_acc: 0.6876\n",
            "Epoch 293/1000\n",
            "1943/1943 [==============================] - 1s 511us/step - loss: 0.7399 - acc: 0.7627 - val_loss: 0.9227 - val_acc: 0.6928\n",
            "Epoch 294/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.7441 - acc: 0.7524 - val_loss: 0.9183 - val_acc: 0.6917\n",
            "Epoch 295/1000\n",
            "1943/1943 [==============================] - 1s 498us/step - loss: 0.7388 - acc: 0.7524 - val_loss: 0.9224 - val_acc: 0.6834\n",
            "Epoch 296/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.7462 - acc: 0.7432 - val_loss: 0.9283 - val_acc: 0.6834\n",
            "Epoch 297/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.7248 - acc: 0.7514 - val_loss: 0.9260 - val_acc: 0.6928\n",
            "Epoch 298/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.7371 - acc: 0.7571 - val_loss: 0.9235 - val_acc: 0.6813\n",
            "Epoch 299/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.7298 - acc: 0.7627 - val_loss: 0.9207 - val_acc: 0.6949\n",
            "Epoch 300/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.7518 - acc: 0.7550 - val_loss: 0.9190 - val_acc: 0.6792\n",
            "Epoch 301/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.7276 - acc: 0.7488 - val_loss: 0.9199 - val_acc: 0.6865\n",
            "Epoch 302/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.7342 - acc: 0.7535 - val_loss: 0.9179 - val_acc: 0.6897\n",
            "Epoch 303/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.7369 - acc: 0.7586 - val_loss: 0.9186 - val_acc: 0.6970\n",
            "Epoch 304/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.7295 - acc: 0.7643 - val_loss: 0.9226 - val_acc: 0.6855\n",
            "Epoch 305/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.7444 - acc: 0.7540 - val_loss: 0.9165 - val_acc: 0.6917\n",
            "Epoch 306/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.7335 - acc: 0.7478 - val_loss: 0.9251 - val_acc: 0.6886\n",
            "Epoch 307/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.7416 - acc: 0.7483 - val_loss: 0.9234 - val_acc: 0.6771\n",
            "Epoch 308/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.7428 - acc: 0.7504 - val_loss: 0.9265 - val_acc: 0.6823\n",
            "Epoch 309/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.7417 - acc: 0.7509 - val_loss: 0.9225 - val_acc: 0.6917\n",
            "Epoch 310/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.7174 - acc: 0.7633 - val_loss: 0.9251 - val_acc: 0.6897\n",
            "Epoch 311/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.7301 - acc: 0.7463 - val_loss: 0.9205 - val_acc: 0.7043\n",
            "Epoch 312/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.7379 - acc: 0.7514 - val_loss: 0.9215 - val_acc: 0.6782\n",
            "Epoch 313/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.7410 - acc: 0.7633 - val_loss: 0.9179 - val_acc: 0.6897\n",
            "Epoch 314/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.7260 - acc: 0.7540 - val_loss: 0.9165 - val_acc: 0.6949\n",
            "Epoch 315/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.7211 - acc: 0.7617 - val_loss: 0.9146 - val_acc: 0.6917\n",
            "Epoch 316/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.7198 - acc: 0.7571 - val_loss: 0.9159 - val_acc: 0.6771\n",
            "Epoch 317/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.7373 - acc: 0.7504 - val_loss: 0.9146 - val_acc: 0.7001\n",
            "Epoch 318/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.7192 - acc: 0.7571 - val_loss: 0.9128 - val_acc: 0.6928\n",
            "Epoch 319/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.7401 - acc: 0.7478 - val_loss: 0.9133 - val_acc: 0.6928\n",
            "Epoch 320/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.7336 - acc: 0.7509 - val_loss: 0.9147 - val_acc: 0.6970\n",
            "Epoch 321/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.7410 - acc: 0.7473 - val_loss: 0.9142 - val_acc: 0.6907\n",
            "Epoch 322/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.7288 - acc: 0.7545 - val_loss: 0.9125 - val_acc: 0.6928\n",
            "Epoch 323/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.7117 - acc: 0.7622 - val_loss: 0.9194 - val_acc: 0.6844\n",
            "Epoch 324/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.7410 - acc: 0.7499 - val_loss: 0.9147 - val_acc: 0.6876\n",
            "Epoch 325/1000\n",
            "1943/1943 [==============================] - 1s 502us/step - loss: 0.7449 - acc: 0.7494 - val_loss: 0.9163 - val_acc: 0.6928\n",
            "Epoch 326/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.7390 - acc: 0.7524 - val_loss: 0.9158 - val_acc: 0.7022\n",
            "Epoch 327/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.7329 - acc: 0.7555 - val_loss: 0.9125 - val_acc: 0.6876\n",
            "Epoch 328/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.7129 - acc: 0.7658 - val_loss: 0.9155 - val_acc: 0.6970\n",
            "Epoch 329/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.7269 - acc: 0.7483 - val_loss: 0.9125 - val_acc: 0.6938\n",
            "Epoch 330/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.7241 - acc: 0.7550 - val_loss: 0.9128 - val_acc: 0.6938\n",
            "Epoch 331/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.7191 - acc: 0.7638 - val_loss: 0.9162 - val_acc: 0.6949\n",
            "Epoch 332/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.7059 - acc: 0.7627 - val_loss: 0.9111 - val_acc: 0.6959\n",
            "Epoch 333/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.7369 - acc: 0.7530 - val_loss: 0.9138 - val_acc: 0.7011\n",
            "Epoch 334/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.7337 - acc: 0.7509 - val_loss: 0.9116 - val_acc: 0.7011\n",
            "Epoch 335/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.7091 - acc: 0.7663 - val_loss: 0.9097 - val_acc: 0.6865\n",
            "Epoch 336/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.7219 - acc: 0.7581 - val_loss: 0.9097 - val_acc: 0.7064\n",
            "Epoch 337/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.7193 - acc: 0.7566 - val_loss: 0.9072 - val_acc: 0.6917\n",
            "Epoch 338/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6990 - acc: 0.7746 - val_loss: 0.9116 - val_acc: 0.6938\n",
            "Epoch 339/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.7461 - acc: 0.7380 - val_loss: 0.9124 - val_acc: 0.7085\n",
            "Epoch 340/1000\n",
            "1943/1943 [==============================] - 1s 515us/step - loss: 0.7145 - acc: 0.7643 - val_loss: 0.9101 - val_acc: 0.6938\n",
            "Epoch 341/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.7176 - acc: 0.7617 - val_loss: 0.9101 - val_acc: 0.6938\n",
            "Epoch 342/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.7114 - acc: 0.7674 - val_loss: 0.9108 - val_acc: 0.7106\n",
            "Epoch 343/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.7172 - acc: 0.7597 - val_loss: 0.9122 - val_acc: 0.7011\n",
            "Epoch 344/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.7195 - acc: 0.7535 - val_loss: 0.9061 - val_acc: 0.7032\n",
            "Epoch 345/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.7287 - acc: 0.7581 - val_loss: 0.9061 - val_acc: 0.6970\n",
            "Epoch 346/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.7207 - acc: 0.7524 - val_loss: 0.9100 - val_acc: 0.6897\n",
            "Epoch 347/1000\n",
            "1943/1943 [==============================] - 1s 554us/step - loss: 0.7191 - acc: 0.7622 - val_loss: 0.9102 - val_acc: 0.7001\n",
            "Epoch 348/1000\n",
            "1943/1943 [==============================] - 1s 562us/step - loss: 0.7231 - acc: 0.7514 - val_loss: 0.9117 - val_acc: 0.7064\n",
            "Epoch 349/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.7105 - acc: 0.7684 - val_loss: 0.9061 - val_acc: 0.6959\n",
            "Epoch 350/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.7235 - acc: 0.7627 - val_loss: 0.9044 - val_acc: 0.7064\n",
            "Epoch 351/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.7191 - acc: 0.7766 - val_loss: 0.9106 - val_acc: 0.7032\n",
            "Epoch 352/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.7109 - acc: 0.7679 - val_loss: 0.9051 - val_acc: 0.6970\n",
            "Epoch 353/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.7179 - acc: 0.7669 - val_loss: 0.9052 - val_acc: 0.6970\n",
            "Epoch 354/1000\n",
            "1943/1943 [==============================] - 1s 489us/step - loss: 0.7234 - acc: 0.7509 - val_loss: 0.9055 - val_acc: 0.6991\n",
            "Epoch 355/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.7145 - acc: 0.7566 - val_loss: 0.8990 - val_acc: 0.7053\n",
            "Epoch 356/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.7135 - acc: 0.7658 - val_loss: 0.8997 - val_acc: 0.6980\n",
            "Epoch 357/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.7168 - acc: 0.7597 - val_loss: 0.9037 - val_acc: 0.6917\n",
            "Epoch 358/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.7249 - acc: 0.7535 - val_loss: 0.9017 - val_acc: 0.6980\n",
            "Epoch 359/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.7081 - acc: 0.7751 - val_loss: 0.9035 - val_acc: 0.7011\n",
            "Epoch 360/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.7188 - acc: 0.7524 - val_loss: 0.9003 - val_acc: 0.7095\n",
            "Epoch 361/1000\n",
            "1943/1943 [==============================] - 1s 509us/step - loss: 0.6982 - acc: 0.7725 - val_loss: 0.9094 - val_acc: 0.6991\n",
            "Epoch 362/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.7272 - acc: 0.7545 - val_loss: 0.9029 - val_acc: 0.6959\n",
            "Epoch 363/1000\n",
            "1943/1943 [==============================] - 1s 562us/step - loss: 0.7192 - acc: 0.7540 - val_loss: 0.8986 - val_acc: 0.7053\n",
            "Epoch 364/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.7017 - acc: 0.7766 - val_loss: 0.9014 - val_acc: 0.6949\n",
            "Epoch 365/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.7139 - acc: 0.7627 - val_loss: 0.9025 - val_acc: 0.6970\n",
            "Epoch 366/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.7028 - acc: 0.7653 - val_loss: 0.8996 - val_acc: 0.7001\n",
            "Epoch 367/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.6948 - acc: 0.7710 - val_loss: 0.8982 - val_acc: 0.6970\n",
            "Epoch 368/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.7293 - acc: 0.7566 - val_loss: 0.8984 - val_acc: 0.7106\n",
            "Epoch 369/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.7106 - acc: 0.7643 - val_loss: 0.8957 - val_acc: 0.7011\n",
            "Epoch 370/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.7075 - acc: 0.7622 - val_loss: 0.8986 - val_acc: 0.6991\n",
            "Epoch 371/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.7050 - acc: 0.7633 - val_loss: 0.9035 - val_acc: 0.6991\n",
            "Epoch 372/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.7097 - acc: 0.7617 - val_loss: 0.8984 - val_acc: 0.7011\n",
            "Epoch 373/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.7055 - acc: 0.7725 - val_loss: 0.9052 - val_acc: 0.6949\n",
            "Epoch 374/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6949 - acc: 0.7751 - val_loss: 0.8961 - val_acc: 0.7095\n",
            "Epoch 375/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.7202 - acc: 0.7576 - val_loss: 0.9049 - val_acc: 0.6949\n",
            "Epoch 376/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.6867 - acc: 0.7684 - val_loss: 0.8966 - val_acc: 0.7022\n",
            "Epoch 377/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.6960 - acc: 0.7720 - val_loss: 0.8970 - val_acc: 0.7074\n",
            "Epoch 378/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.7176 - acc: 0.7581 - val_loss: 0.8975 - val_acc: 0.7001\n",
            "Epoch 379/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.7105 - acc: 0.7555 - val_loss: 0.8951 - val_acc: 0.7074\n",
            "Epoch 380/1000\n",
            "1943/1943 [==============================] - 1s 456us/step - loss: 0.7162 - acc: 0.7524 - val_loss: 0.8970 - val_acc: 0.7043\n",
            "Epoch 381/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.7041 - acc: 0.7622 - val_loss: 0.8973 - val_acc: 0.7074\n",
            "Epoch 382/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6974 - acc: 0.7653 - val_loss: 0.8937 - val_acc: 0.7053\n",
            "Epoch 383/1000\n",
            "1943/1943 [==============================] - 1s 463us/step - loss: 0.7051 - acc: 0.7607 - val_loss: 0.8959 - val_acc: 0.6938\n",
            "Epoch 384/1000\n",
            "1943/1943 [==============================] - 1s 515us/step - loss: 0.7071 - acc: 0.7612 - val_loss: 0.8964 - val_acc: 0.7043\n",
            "Epoch 385/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.7065 - acc: 0.7473 - val_loss: 0.8933 - val_acc: 0.6907\n",
            "Epoch 386/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.7025 - acc: 0.7699 - val_loss: 0.8937 - val_acc: 0.7001\n",
            "Epoch 387/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.7102 - acc: 0.7607 - val_loss: 0.8907 - val_acc: 0.7022\n",
            "Epoch 388/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.6945 - acc: 0.7777 - val_loss: 0.8946 - val_acc: 0.7085\n",
            "Epoch 389/1000\n",
            "1943/1943 [==============================] - 1s 505us/step - loss: 0.7035 - acc: 0.7638 - val_loss: 0.8909 - val_acc: 0.6980\n",
            "Epoch 390/1000\n",
            "1943/1943 [==============================] - 1s 502us/step - loss: 0.6895 - acc: 0.7715 - val_loss: 0.8964 - val_acc: 0.7001\n",
            "Epoch 391/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6859 - acc: 0.7669 - val_loss: 0.9001 - val_acc: 0.7001\n",
            "Epoch 392/1000\n",
            "1943/1943 [==============================] - 1s 585us/step - loss: 0.6893 - acc: 0.7710 - val_loss: 0.8925 - val_acc: 0.7095\n",
            "Epoch 393/1000\n",
            "1943/1943 [==============================] - 1s 482us/step - loss: 0.6934 - acc: 0.7669 - val_loss: 0.8986 - val_acc: 0.7011\n",
            "Epoch 394/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.7077 - acc: 0.7622 - val_loss: 0.8895 - val_acc: 0.7074\n",
            "Epoch 395/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.7122 - acc: 0.7555 - val_loss: 0.8958 - val_acc: 0.6991\n",
            "Epoch 396/1000\n",
            "1943/1943 [==============================] - 1s 467us/step - loss: 0.6986 - acc: 0.7653 - val_loss: 0.8945 - val_acc: 0.7032\n",
            "Epoch 397/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6778 - acc: 0.7777 - val_loss: 0.8941 - val_acc: 0.7168\n",
            "Epoch 398/1000\n",
            "1943/1943 [==============================] - 1s 494us/step - loss: 0.6903 - acc: 0.7669 - val_loss: 0.8985 - val_acc: 0.7022\n",
            "Epoch 399/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.7090 - acc: 0.7633 - val_loss: 0.8904 - val_acc: 0.7053\n",
            "Epoch 400/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.6985 - acc: 0.7658 - val_loss: 0.8929 - val_acc: 0.6980\n",
            "Epoch 401/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.6929 - acc: 0.7669 - val_loss: 0.8909 - val_acc: 0.7011\n",
            "Epoch 402/1000\n",
            "1943/1943 [==============================] - 1s 572us/step - loss: 0.6966 - acc: 0.7653 - val_loss: 0.8882 - val_acc: 0.7189\n",
            "Epoch 403/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.6815 - acc: 0.7730 - val_loss: 0.8877 - val_acc: 0.7022\n",
            "Epoch 404/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.6815 - acc: 0.7880 - val_loss: 0.8943 - val_acc: 0.7001\n",
            "Epoch 405/1000\n",
            "1943/1943 [==============================] - 1s 586us/step - loss: 0.6991 - acc: 0.7633 - val_loss: 0.8955 - val_acc: 0.6938\n",
            "Epoch 406/1000\n",
            "1943/1943 [==============================] - 1s 580us/step - loss: 0.6934 - acc: 0.7602 - val_loss: 0.8901 - val_acc: 0.7085\n",
            "Epoch 407/1000\n",
            "1943/1943 [==============================] - 1s 591us/step - loss: 0.7090 - acc: 0.7746 - val_loss: 0.8905 - val_acc: 0.7095\n",
            "Epoch 408/1000\n",
            "1943/1943 [==============================] - 1s 590us/step - loss: 0.6857 - acc: 0.7633 - val_loss: 0.8895 - val_acc: 0.7043\n",
            "Epoch 409/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6804 - acc: 0.7710 - val_loss: 0.8881 - val_acc: 0.7011\n",
            "Epoch 410/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6888 - acc: 0.7777 - val_loss: 0.8882 - val_acc: 0.7064\n",
            "Epoch 411/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.6701 - acc: 0.7699 - val_loss: 0.8993 - val_acc: 0.6949\n",
            "Epoch 412/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6968 - acc: 0.7669 - val_loss: 0.8887 - val_acc: 0.7085\n",
            "Epoch 413/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6838 - acc: 0.7797 - val_loss: 0.8829 - val_acc: 0.7106\n",
            "Epoch 414/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6949 - acc: 0.7730 - val_loss: 0.8847 - val_acc: 0.6980\n",
            "Epoch 415/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6731 - acc: 0.7838 - val_loss: 0.8878 - val_acc: 0.7095\n",
            "Epoch 416/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.6881 - acc: 0.7730 - val_loss: 0.8827 - val_acc: 0.7074\n",
            "Epoch 417/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.6911 - acc: 0.7633 - val_loss: 0.8844 - val_acc: 0.7126\n",
            "Epoch 418/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6778 - acc: 0.7746 - val_loss: 0.8838 - val_acc: 0.7126\n",
            "Epoch 419/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.6876 - acc: 0.7658 - val_loss: 0.8855 - val_acc: 0.7032\n",
            "Epoch 420/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.7006 - acc: 0.7581 - val_loss: 0.8881 - val_acc: 0.6980\n",
            "Epoch 421/1000\n",
            "1943/1943 [==============================] - 1s 490us/step - loss: 0.6824 - acc: 0.7797 - val_loss: 0.8867 - val_acc: 0.7074\n",
            "Epoch 422/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.6969 - acc: 0.7633 - val_loss: 0.8866 - val_acc: 0.7064\n",
            "Epoch 423/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6987 - acc: 0.7710 - val_loss: 0.8870 - val_acc: 0.7064\n",
            "Epoch 424/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.7109 - acc: 0.7689 - val_loss: 0.8887 - val_acc: 0.7053\n",
            "Epoch 425/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6862 - acc: 0.7674 - val_loss: 0.8800 - val_acc: 0.7053\n",
            "Epoch 426/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6861 - acc: 0.7699 - val_loss: 0.8844 - val_acc: 0.7158\n",
            "Epoch 427/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.6708 - acc: 0.7859 - val_loss: 0.8826 - val_acc: 0.7011\n",
            "Epoch 428/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6695 - acc: 0.7735 - val_loss: 0.8815 - val_acc: 0.7126\n",
            "Epoch 429/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.6853 - acc: 0.7756 - val_loss: 0.8774 - val_acc: 0.7043\n",
            "Epoch 430/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6810 - acc: 0.7679 - val_loss: 0.8855 - val_acc: 0.7001\n",
            "Epoch 431/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.7023 - acc: 0.7699 - val_loss: 0.8782 - val_acc: 0.7179\n",
            "Epoch 432/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.6678 - acc: 0.7823 - val_loss: 0.8820 - val_acc: 0.7137\n",
            "Epoch 433/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.6833 - acc: 0.7705 - val_loss: 0.8855 - val_acc: 0.7168\n",
            "Epoch 434/1000\n",
            "1943/1943 [==============================] - 1s 473us/step - loss: 0.6643 - acc: 0.7916 - val_loss: 0.8775 - val_acc: 0.7158\n",
            "Epoch 435/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.6623 - acc: 0.7895 - val_loss: 0.8879 - val_acc: 0.7106\n",
            "Epoch 436/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6773 - acc: 0.7792 - val_loss: 0.8784 - val_acc: 0.7200\n",
            "Epoch 437/1000\n",
            "1943/1943 [==============================] - 1s 511us/step - loss: 0.6664 - acc: 0.7766 - val_loss: 0.8787 - val_acc: 0.7043\n",
            "Epoch 438/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6839 - acc: 0.7679 - val_loss: 0.8752 - val_acc: 0.7085\n",
            "Epoch 439/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.6767 - acc: 0.7797 - val_loss: 0.8790 - val_acc: 0.7231\n",
            "Epoch 440/1000\n",
            "1943/1943 [==============================] - 1s 496us/step - loss: 0.6844 - acc: 0.7720 - val_loss: 0.8729 - val_acc: 0.7095\n",
            "Epoch 441/1000\n",
            "1943/1943 [==============================] - 1s 484us/step - loss: 0.6979 - acc: 0.7735 - val_loss: 0.8765 - val_acc: 0.7137\n",
            "Epoch 442/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6770 - acc: 0.7679 - val_loss: 0.8760 - val_acc: 0.7116\n",
            "Epoch 443/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.7097 - acc: 0.7643 - val_loss: 0.8788 - val_acc: 0.7137\n",
            "Epoch 444/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6833 - acc: 0.7746 - val_loss: 0.8778 - val_acc: 0.7085\n",
            "Epoch 445/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.6887 - acc: 0.7653 - val_loss: 0.8749 - val_acc: 0.7137\n",
            "Epoch 446/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.6776 - acc: 0.7710 - val_loss: 0.8754 - val_acc: 0.7231\n",
            "Epoch 447/1000\n",
            "1943/1943 [==============================] - 1s 495us/step - loss: 0.6590 - acc: 0.7880 - val_loss: 0.8770 - val_acc: 0.7064\n",
            "Epoch 448/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.6653 - acc: 0.7880 - val_loss: 0.8783 - val_acc: 0.7106\n",
            "Epoch 449/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.6810 - acc: 0.7751 - val_loss: 0.8837 - val_acc: 0.7126\n",
            "Epoch 450/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6692 - acc: 0.7715 - val_loss: 0.8821 - val_acc: 0.7179\n",
            "Epoch 451/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.6802 - acc: 0.7705 - val_loss: 0.8739 - val_acc: 0.7158\n",
            "Epoch 452/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6781 - acc: 0.7638 - val_loss: 0.8759 - val_acc: 0.7189\n",
            "Epoch 453/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6668 - acc: 0.7813 - val_loss: 0.8767 - val_acc: 0.7095\n",
            "Epoch 454/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6810 - acc: 0.7746 - val_loss: 0.8733 - val_acc: 0.7168\n",
            "Epoch 455/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.6598 - acc: 0.7818 - val_loss: 0.8728 - val_acc: 0.7168\n",
            "Epoch 456/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.6781 - acc: 0.7741 - val_loss: 0.8699 - val_acc: 0.7137\n",
            "Epoch 457/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6687 - acc: 0.7833 - val_loss: 0.8764 - val_acc: 0.7095\n",
            "Epoch 458/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.6671 - acc: 0.7766 - val_loss: 0.8737 - val_acc: 0.7095\n",
            "Epoch 459/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6643 - acc: 0.7705 - val_loss: 0.8688 - val_acc: 0.7158\n",
            "Epoch 460/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6762 - acc: 0.7684 - val_loss: 0.8719 - val_acc: 0.7220\n",
            "Epoch 461/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.6830 - acc: 0.7694 - val_loss: 0.8683 - val_acc: 0.7210\n",
            "Epoch 462/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6729 - acc: 0.7741 - val_loss: 0.8712 - val_acc: 0.7126\n",
            "Epoch 463/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6741 - acc: 0.7705 - val_loss: 0.8832 - val_acc: 0.7179\n",
            "Epoch 464/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.6787 - acc: 0.7689 - val_loss: 0.8679 - val_acc: 0.7116\n",
            "Epoch 465/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.6813 - acc: 0.7633 - val_loss: 0.8700 - val_acc: 0.7241\n",
            "Epoch 466/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.6707 - acc: 0.7766 - val_loss: 0.8707 - val_acc: 0.7106\n",
            "Epoch 467/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6678 - acc: 0.7746 - val_loss: 0.8700 - val_acc: 0.7137\n",
            "Epoch 468/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.6414 - acc: 0.7854 - val_loss: 0.8702 - val_acc: 0.7168\n",
            "Epoch 469/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6625 - acc: 0.7792 - val_loss: 0.8700 - val_acc: 0.7210\n",
            "Epoch 470/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.6702 - acc: 0.7802 - val_loss: 0.8741 - val_acc: 0.7179\n",
            "Epoch 471/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6589 - acc: 0.7854 - val_loss: 0.8719 - val_acc: 0.7189\n",
            "Epoch 472/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6684 - acc: 0.7766 - val_loss: 0.8713 - val_acc: 0.7147\n",
            "Epoch 473/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6739 - acc: 0.7787 - val_loss: 0.8708 - val_acc: 0.7179\n",
            "Epoch 474/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.6640 - acc: 0.7689 - val_loss: 0.8747 - val_acc: 0.7158\n",
            "Epoch 475/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6681 - acc: 0.7849 - val_loss: 0.8661 - val_acc: 0.7168\n",
            "Epoch 476/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.6576 - acc: 0.7838 - val_loss: 0.8678 - val_acc: 0.7126\n",
            "Epoch 477/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6729 - acc: 0.7705 - val_loss: 0.8646 - val_acc: 0.7158\n",
            "Epoch 478/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6673 - acc: 0.7766 - val_loss: 0.8709 - val_acc: 0.7147\n",
            "Epoch 479/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.6716 - acc: 0.7792 - val_loss: 0.8681 - val_acc: 0.7147\n",
            "Epoch 480/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6516 - acc: 0.7823 - val_loss: 0.8611 - val_acc: 0.7137\n",
            "Epoch 481/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.6427 - acc: 0.7823 - val_loss: 0.8644 - val_acc: 0.7158\n",
            "Epoch 482/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6711 - acc: 0.7627 - val_loss: 0.8649 - val_acc: 0.7189\n",
            "Epoch 483/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.6464 - acc: 0.7926 - val_loss: 0.8646 - val_acc: 0.7137\n",
            "Epoch 484/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6444 - acc: 0.7808 - val_loss: 0.8679 - val_acc: 0.7168\n",
            "Epoch 485/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6491 - acc: 0.7962 - val_loss: 0.8644 - val_acc: 0.7168\n",
            "Epoch 486/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6609 - acc: 0.7844 - val_loss: 0.8635 - val_acc: 0.7137\n",
            "Epoch 487/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6667 - acc: 0.7808 - val_loss: 0.8617 - val_acc: 0.7220\n",
            "Epoch 488/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6464 - acc: 0.7833 - val_loss: 0.8620 - val_acc: 0.7252\n",
            "Epoch 489/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6644 - acc: 0.7859 - val_loss: 0.8578 - val_acc: 0.7273\n",
            "Epoch 490/1000\n",
            "1943/1943 [==============================] - 1s 574us/step - loss: 0.6668 - acc: 0.7746 - val_loss: 0.8596 - val_acc: 0.7220\n",
            "Epoch 491/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.6628 - acc: 0.7823 - val_loss: 0.8551 - val_acc: 0.7220\n",
            "Epoch 492/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.6555 - acc: 0.7782 - val_loss: 0.8573 - val_acc: 0.7210\n",
            "Epoch 493/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6767 - acc: 0.7735 - val_loss: 0.8648 - val_acc: 0.7168\n",
            "Epoch 494/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6548 - acc: 0.7761 - val_loss: 0.8577 - val_acc: 0.7241\n",
            "Epoch 495/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6674 - acc: 0.7720 - val_loss: 0.8562 - val_acc: 0.7220\n",
            "Epoch 496/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6599 - acc: 0.7813 - val_loss: 0.8569 - val_acc: 0.7200\n",
            "Epoch 497/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.6427 - acc: 0.7916 - val_loss: 0.8557 - val_acc: 0.7262\n",
            "Epoch 498/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.6685 - acc: 0.7720 - val_loss: 0.8586 - val_acc: 0.7231\n",
            "Epoch 499/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.6490 - acc: 0.7818 - val_loss: 0.8627 - val_acc: 0.7210\n",
            "Epoch 500/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.6588 - acc: 0.7885 - val_loss: 0.8559 - val_acc: 0.7252\n",
            "Epoch 501/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6425 - acc: 0.7926 - val_loss: 0.8557 - val_acc: 0.7241\n",
            "Epoch 502/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.6419 - acc: 0.7895 - val_loss: 0.8598 - val_acc: 0.7241\n",
            "Epoch 503/1000\n",
            "1943/1943 [==============================] - 1s 580us/step - loss: 0.6404 - acc: 0.7869 - val_loss: 0.8539 - val_acc: 0.7210\n",
            "Epoch 504/1000\n",
            "1943/1943 [==============================] - 1s 574us/step - loss: 0.6511 - acc: 0.7833 - val_loss: 0.8548 - val_acc: 0.7220\n",
            "Epoch 505/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.6622 - acc: 0.7751 - val_loss: 0.8545 - val_acc: 0.7179\n",
            "Epoch 506/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6514 - acc: 0.7921 - val_loss: 0.8565 - val_acc: 0.7241\n",
            "Epoch 507/1000\n",
            "1943/1943 [==============================] - 1s 586us/step - loss: 0.6464 - acc: 0.7782 - val_loss: 0.8528 - val_acc: 0.7179\n",
            "Epoch 508/1000\n",
            "1943/1943 [==============================] - 1s 562us/step - loss: 0.6508 - acc: 0.7854 - val_loss: 0.8556 - val_acc: 0.7241\n",
            "Epoch 509/1000\n",
            "1943/1943 [==============================] - 1s 580us/step - loss: 0.6550 - acc: 0.7808 - val_loss: 0.8620 - val_acc: 0.7210\n",
            "Epoch 510/1000\n",
            "1943/1943 [==============================] - 1s 577us/step - loss: 0.6625 - acc: 0.7910 - val_loss: 0.8530 - val_acc: 0.7262\n",
            "Epoch 511/1000\n",
            "1943/1943 [==============================] - 1s 585us/step - loss: 0.6345 - acc: 0.7993 - val_loss: 0.8525 - val_acc: 0.7241\n",
            "Epoch 512/1000\n",
            "1943/1943 [==============================] - 1s 571us/step - loss: 0.6535 - acc: 0.7818 - val_loss: 0.8499 - val_acc: 0.7273\n",
            "Epoch 513/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6598 - acc: 0.7802 - val_loss: 0.8527 - val_acc: 0.7189\n",
            "Epoch 514/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.6508 - acc: 0.7771 - val_loss: 0.8532 - val_acc: 0.7273\n",
            "Epoch 515/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.6597 - acc: 0.7725 - val_loss: 0.8537 - val_acc: 0.7252\n",
            "Epoch 516/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.6494 - acc: 0.7849 - val_loss: 0.8561 - val_acc: 0.7210\n",
            "Epoch 517/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6535 - acc: 0.7828 - val_loss: 0.8554 - val_acc: 0.7262\n",
            "Epoch 518/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.6566 - acc: 0.7792 - val_loss: 0.8506 - val_acc: 0.7200\n",
            "Epoch 519/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.6509 - acc: 0.7880 - val_loss: 0.8512 - val_acc: 0.7241\n",
            "Epoch 520/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6586 - acc: 0.7741 - val_loss: 0.8494 - val_acc: 0.7294\n",
            "Epoch 521/1000\n",
            "1943/1943 [==============================] - 1s 581us/step - loss: 0.6569 - acc: 0.7900 - val_loss: 0.8530 - val_acc: 0.7220\n",
            "Epoch 522/1000\n",
            "1943/1943 [==============================] - 1s 575us/step - loss: 0.6503 - acc: 0.7844 - val_loss: 0.8483 - val_acc: 0.7252\n",
            "Epoch 523/1000\n",
            "1943/1943 [==============================] - 1s 566us/step - loss: 0.6428 - acc: 0.7802 - val_loss: 0.8534 - val_acc: 0.7200\n",
            "Epoch 524/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6504 - acc: 0.7838 - val_loss: 0.8463 - val_acc: 0.7179\n",
            "Epoch 525/1000\n",
            "1943/1943 [==============================] - 1s 594us/step - loss: 0.6390 - acc: 0.7931 - val_loss: 0.8527 - val_acc: 0.7273\n",
            "Epoch 526/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6310 - acc: 0.7936 - val_loss: 0.8541 - val_acc: 0.7252\n",
            "Epoch 527/1000\n",
            "1943/1943 [==============================] - 1s 553us/step - loss: 0.6284 - acc: 0.7890 - val_loss: 0.8478 - val_acc: 0.7220\n",
            "Epoch 528/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.6420 - acc: 0.7921 - val_loss: 0.8477 - val_acc: 0.7241\n",
            "Epoch 529/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6589 - acc: 0.7751 - val_loss: 0.8499 - val_acc: 0.7200\n",
            "Epoch 530/1000\n",
            "1943/1943 [==============================] - 1s 553us/step - loss: 0.6510 - acc: 0.7849 - val_loss: 0.8454 - val_acc: 0.7335\n",
            "Epoch 531/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.6435 - acc: 0.7905 - val_loss: 0.8479 - val_acc: 0.7231\n",
            "Epoch 532/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.6534 - acc: 0.7885 - val_loss: 0.8463 - val_acc: 0.7220\n",
            "Epoch 533/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.6192 - acc: 0.8044 - val_loss: 0.8448 - val_acc: 0.7252\n",
            "Epoch 534/1000\n",
            "1943/1943 [==============================] - 1s 566us/step - loss: 0.6520 - acc: 0.7874 - val_loss: 0.8454 - val_acc: 0.7283\n",
            "Epoch 535/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.6441 - acc: 0.7756 - val_loss: 0.8461 - val_acc: 0.7304\n",
            "Epoch 536/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6367 - acc: 0.7941 - val_loss: 0.8480 - val_acc: 0.7283\n",
            "Epoch 537/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6585 - acc: 0.7802 - val_loss: 0.8448 - val_acc: 0.7262\n",
            "Epoch 538/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6228 - acc: 0.7910 - val_loss: 0.8484 - val_acc: 0.7262\n",
            "Epoch 539/1000\n",
            "1943/1943 [==============================] - 1s 576us/step - loss: 0.6404 - acc: 0.7977 - val_loss: 0.8422 - val_acc: 0.7262\n",
            "Epoch 540/1000\n",
            "1943/1943 [==============================] - 1s 579us/step - loss: 0.6312 - acc: 0.8008 - val_loss: 0.8462 - val_acc: 0.7283\n",
            "Epoch 541/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6406 - acc: 0.7926 - val_loss: 0.8470 - val_acc: 0.7252\n",
            "Epoch 542/1000\n",
            "1943/1943 [==============================] - 1s 562us/step - loss: 0.6511 - acc: 0.7838 - val_loss: 0.8437 - val_acc: 0.7220\n",
            "Epoch 543/1000\n",
            "1943/1943 [==============================] - 1s 566us/step - loss: 0.6367 - acc: 0.7946 - val_loss: 0.8410 - val_acc: 0.7262\n",
            "Epoch 544/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.6544 - acc: 0.7808 - val_loss: 0.8452 - val_acc: 0.7304\n",
            "Epoch 545/1000\n",
            "1943/1943 [==============================] - 1s 576us/step - loss: 0.6456 - acc: 0.7854 - val_loss: 0.8482 - val_acc: 0.7231\n",
            "Epoch 546/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6368 - acc: 0.7926 - val_loss: 0.8446 - val_acc: 0.7273\n",
            "Epoch 547/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6533 - acc: 0.7787 - val_loss: 0.8493 - val_acc: 0.7294\n",
            "Epoch 548/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.6352 - acc: 0.7910 - val_loss: 0.8416 - val_acc: 0.7241\n",
            "Epoch 549/1000\n",
            "1943/1943 [==============================] - 1s 577us/step - loss: 0.6401 - acc: 0.7972 - val_loss: 0.8494 - val_acc: 0.7262\n",
            "Epoch 550/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.6440 - acc: 0.7808 - val_loss: 0.8498 - val_acc: 0.7315\n",
            "Epoch 551/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.6271 - acc: 0.7905 - val_loss: 0.8468 - val_acc: 0.7273\n",
            "Epoch 552/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.6280 - acc: 0.7946 - val_loss: 0.8414 - val_acc: 0.7241\n",
            "Epoch 553/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6512 - acc: 0.7833 - val_loss: 0.8433 - val_acc: 0.7262\n",
            "Epoch 554/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6363 - acc: 0.7900 - val_loss: 0.8500 - val_acc: 0.7179\n",
            "Epoch 555/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6418 - acc: 0.7957 - val_loss: 0.8365 - val_acc: 0.7294\n",
            "Epoch 556/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6373 - acc: 0.7828 - val_loss: 0.8392 - val_acc: 0.7356\n",
            "Epoch 557/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6197 - acc: 0.7972 - val_loss: 0.8413 - val_acc: 0.7335\n",
            "Epoch 558/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.6331 - acc: 0.7967 - val_loss: 0.8390 - val_acc: 0.7335\n",
            "Epoch 559/1000\n",
            "1943/1943 [==============================] - 1s 562us/step - loss: 0.6583 - acc: 0.7854 - val_loss: 0.8433 - val_acc: 0.7262\n",
            "Epoch 560/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.6469 - acc: 0.7854 - val_loss: 0.8448 - val_acc: 0.7304\n",
            "Epoch 561/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.6430 - acc: 0.7983 - val_loss: 0.8425 - val_acc: 0.7325\n",
            "Epoch 562/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6384 - acc: 0.7869 - val_loss: 0.8398 - val_acc: 0.7398\n",
            "Epoch 563/1000\n",
            "1943/1943 [==============================] - 1s 553us/step - loss: 0.6343 - acc: 0.7885 - val_loss: 0.8368 - val_acc: 0.7388\n",
            "Epoch 564/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6378 - acc: 0.7838 - val_loss: 0.8393 - val_acc: 0.7346\n",
            "Epoch 565/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6283 - acc: 0.7998 - val_loss: 0.8353 - val_acc: 0.7367\n",
            "Epoch 566/1000\n",
            "1943/1943 [==============================] - 1s 515us/step - loss: 0.6293 - acc: 0.7941 - val_loss: 0.8365 - val_acc: 0.7335\n",
            "Epoch 567/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.6094 - acc: 0.7926 - val_loss: 0.8350 - val_acc: 0.7304\n",
            "Epoch 568/1000\n",
            "1943/1943 [==============================] - 1s 511us/step - loss: 0.6201 - acc: 0.7952 - val_loss: 0.8351 - val_acc: 0.7283\n",
            "Epoch 569/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6342 - acc: 0.7941 - val_loss: 0.8386 - val_acc: 0.7273\n",
            "Epoch 570/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.6286 - acc: 0.7921 - val_loss: 0.8309 - val_acc: 0.7346\n",
            "Epoch 571/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6344 - acc: 0.7926 - val_loss: 0.8312 - val_acc: 0.7356\n",
            "Epoch 572/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.6341 - acc: 0.7874 - val_loss: 0.8370 - val_acc: 0.7315\n",
            "Epoch 573/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.6214 - acc: 0.8029 - val_loss: 0.8326 - val_acc: 0.7356\n",
            "Epoch 574/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6324 - acc: 0.7880 - val_loss: 0.8332 - val_acc: 0.7367\n",
            "Epoch 575/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6201 - acc: 0.7905 - val_loss: 0.8334 - val_acc: 0.7346\n",
            "Epoch 576/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.6359 - acc: 0.7900 - val_loss: 0.8352 - val_acc: 0.7346\n",
            "Epoch 577/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.6291 - acc: 0.7952 - val_loss: 0.8348 - val_acc: 0.7315\n",
            "Epoch 578/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.6203 - acc: 0.8003 - val_loss: 0.8376 - val_acc: 0.7335\n",
            "Epoch 579/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.6321 - acc: 0.7910 - val_loss: 0.8344 - val_acc: 0.7283\n",
            "Epoch 580/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6348 - acc: 0.7885 - val_loss: 0.8421 - val_acc: 0.7252\n",
            "Epoch 581/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.6230 - acc: 0.7988 - val_loss: 0.8381 - val_acc: 0.7346\n",
            "Epoch 582/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.6298 - acc: 0.7972 - val_loss: 0.8363 - val_acc: 0.7356\n",
            "Epoch 583/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6217 - acc: 0.7890 - val_loss: 0.8400 - val_acc: 0.7179\n",
            "Epoch 584/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.6384 - acc: 0.7782 - val_loss: 0.8370 - val_acc: 0.7367\n",
            "Epoch 585/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.6345 - acc: 0.7864 - val_loss: 0.8359 - val_acc: 0.7377\n",
            "Epoch 586/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.6169 - acc: 0.7972 - val_loss: 0.8363 - val_acc: 0.7262\n",
            "Epoch 587/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6305 - acc: 0.7859 - val_loss: 0.8292 - val_acc: 0.7335\n",
            "Epoch 588/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.6166 - acc: 0.7972 - val_loss: 0.8287 - val_acc: 0.7189\n",
            "Epoch 589/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.6343 - acc: 0.7895 - val_loss: 0.8315 - val_acc: 0.7304\n",
            "Epoch 590/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6360 - acc: 0.7910 - val_loss: 0.8287 - val_acc: 0.7398\n",
            "Epoch 591/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6236 - acc: 0.7844 - val_loss: 0.8286 - val_acc: 0.7346\n",
            "Epoch 592/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.6214 - acc: 0.7993 - val_loss: 0.8265 - val_acc: 0.7325\n",
            "Epoch 593/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.6267 - acc: 0.7926 - val_loss: 0.8296 - val_acc: 0.7315\n",
            "Epoch 594/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.6232 - acc: 0.8003 - val_loss: 0.8329 - val_acc: 0.7273\n",
            "Epoch 595/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.6104 - acc: 0.8024 - val_loss: 0.8284 - val_acc: 0.7367\n",
            "Epoch 596/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.6151 - acc: 0.7972 - val_loss: 0.8292 - val_acc: 0.7429\n",
            "Epoch 597/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.6193 - acc: 0.7957 - val_loss: 0.8256 - val_acc: 0.7367\n",
            "Epoch 598/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.6053 - acc: 0.8070 - val_loss: 0.8365 - val_acc: 0.7367\n",
            "Epoch 599/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.6006 - acc: 0.8024 - val_loss: 0.8247 - val_acc: 0.7367\n",
            "Epoch 600/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6099 - acc: 0.7895 - val_loss: 0.8258 - val_acc: 0.7367\n",
            "Epoch 601/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6134 - acc: 0.7936 - val_loss: 0.8312 - val_acc: 0.7335\n",
            "Epoch 602/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6037 - acc: 0.8060 - val_loss: 0.8269 - val_acc: 0.7388\n",
            "Epoch 603/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.6156 - acc: 0.7977 - val_loss: 0.8321 - val_acc: 0.7346\n",
            "Epoch 604/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6176 - acc: 0.7957 - val_loss: 0.8282 - val_acc: 0.7325\n",
            "Epoch 605/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6121 - acc: 0.8003 - val_loss: 0.8312 - val_acc: 0.7356\n",
            "Epoch 606/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6102 - acc: 0.7931 - val_loss: 0.8253 - val_acc: 0.7335\n",
            "Epoch 607/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.6114 - acc: 0.7936 - val_loss: 0.8308 - val_acc: 0.7325\n",
            "Epoch 608/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.6125 - acc: 0.8044 - val_loss: 0.8232 - val_acc: 0.7335\n",
            "Epoch 609/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.6138 - acc: 0.8003 - val_loss: 0.8184 - val_acc: 0.7450\n",
            "Epoch 610/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6094 - acc: 0.8024 - val_loss: 0.8230 - val_acc: 0.7409\n",
            "Epoch 611/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6040 - acc: 0.8024 - val_loss: 0.8234 - val_acc: 0.7356\n",
            "Epoch 612/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6043 - acc: 0.8065 - val_loss: 0.8239 - val_acc: 0.7440\n",
            "Epoch 613/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6029 - acc: 0.8008 - val_loss: 0.8253 - val_acc: 0.7367\n",
            "Epoch 614/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.6221 - acc: 0.7905 - val_loss: 0.8217 - val_acc: 0.7492\n",
            "Epoch 615/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6181 - acc: 0.8044 - val_loss: 0.8237 - val_acc: 0.7356\n",
            "Epoch 616/1000\n",
            "1943/1943 [==============================] - 1s 506us/step - loss: 0.6070 - acc: 0.8049 - val_loss: 0.8206 - val_acc: 0.7419\n",
            "Epoch 617/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6270 - acc: 0.7916 - val_loss: 0.8227 - val_acc: 0.7377\n",
            "Epoch 618/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.6116 - acc: 0.7946 - val_loss: 0.8222 - val_acc: 0.7409\n",
            "Epoch 619/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.6034 - acc: 0.7993 - val_loss: 0.8227 - val_acc: 0.7409\n",
            "Epoch 620/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.6074 - acc: 0.8003 - val_loss: 0.8258 - val_acc: 0.7367\n",
            "Epoch 621/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.6057 - acc: 0.7998 - val_loss: 0.8229 - val_acc: 0.7367\n",
            "Epoch 622/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6369 - acc: 0.7916 - val_loss: 0.8255 - val_acc: 0.7398\n",
            "Epoch 623/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.6225 - acc: 0.7874 - val_loss: 0.8191 - val_acc: 0.7440\n",
            "Epoch 624/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.6151 - acc: 0.7849 - val_loss: 0.8216 - val_acc: 0.7419\n",
            "Epoch 625/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5984 - acc: 0.8034 - val_loss: 0.8223 - val_acc: 0.7450\n",
            "Epoch 626/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.6045 - acc: 0.7967 - val_loss: 0.8186 - val_acc: 0.7398\n",
            "Epoch 627/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.6044 - acc: 0.8034 - val_loss: 0.8238 - val_acc: 0.7377\n",
            "Epoch 628/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6012 - acc: 0.7993 - val_loss: 0.8190 - val_acc: 0.7409\n",
            "Epoch 629/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.6044 - acc: 0.8003 - val_loss: 0.8227 - val_acc: 0.7388\n",
            "Epoch 630/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.6072 - acc: 0.7926 - val_loss: 0.8287 - val_acc: 0.7367\n",
            "Epoch 631/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.5959 - acc: 0.8127 - val_loss: 0.8208 - val_acc: 0.7482\n",
            "Epoch 632/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.6082 - acc: 0.8065 - val_loss: 0.8189 - val_acc: 0.7461\n",
            "Epoch 633/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.6100 - acc: 0.7988 - val_loss: 0.8187 - val_acc: 0.7419\n",
            "Epoch 634/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.6068 - acc: 0.8091 - val_loss: 0.8277 - val_acc: 0.7356\n",
            "Epoch 635/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.6257 - acc: 0.7869 - val_loss: 0.8194 - val_acc: 0.7419\n",
            "Epoch 636/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.6025 - acc: 0.7946 - val_loss: 0.8232 - val_acc: 0.7440\n",
            "Epoch 637/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.6167 - acc: 0.7946 - val_loss: 0.8153 - val_acc: 0.7409\n",
            "Epoch 638/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.6040 - acc: 0.8085 - val_loss: 0.8191 - val_acc: 0.7346\n",
            "Epoch 639/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5994 - acc: 0.8111 - val_loss: 0.8175 - val_acc: 0.7503\n",
            "Epoch 640/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.6088 - acc: 0.7993 - val_loss: 0.8235 - val_acc: 0.7419\n",
            "Epoch 641/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5972 - acc: 0.7993 - val_loss: 0.8170 - val_acc: 0.7419\n",
            "Epoch 642/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6037 - acc: 0.7936 - val_loss: 0.8140 - val_acc: 0.7461\n",
            "Epoch 643/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5972 - acc: 0.8024 - val_loss: 0.8138 - val_acc: 0.7429\n",
            "Epoch 644/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6046 - acc: 0.8034 - val_loss: 0.8180 - val_acc: 0.7471\n",
            "Epoch 645/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.5977 - acc: 0.7972 - val_loss: 0.8199 - val_acc: 0.7388\n",
            "Epoch 646/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.6039 - acc: 0.7998 - val_loss: 0.8204 - val_acc: 0.7492\n",
            "Epoch 647/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.6144 - acc: 0.7972 - val_loss: 0.8173 - val_acc: 0.7461\n",
            "Epoch 648/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.6073 - acc: 0.7916 - val_loss: 0.8124 - val_acc: 0.7367\n",
            "Epoch 649/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.6281 - acc: 0.7885 - val_loss: 0.8130 - val_acc: 0.7440\n",
            "Epoch 650/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5873 - acc: 0.8127 - val_loss: 0.8163 - val_acc: 0.7471\n",
            "Epoch 651/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5918 - acc: 0.8137 - val_loss: 0.8123 - val_acc: 0.7429\n",
            "Epoch 652/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5984 - acc: 0.8003 - val_loss: 0.8144 - val_acc: 0.7471\n",
            "Epoch 653/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.6055 - acc: 0.8024 - val_loss: 0.8135 - val_acc: 0.7450\n",
            "Epoch 654/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.6013 - acc: 0.7993 - val_loss: 0.8092 - val_acc: 0.7524\n",
            "Epoch 655/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5821 - acc: 0.8240 - val_loss: 0.8078 - val_acc: 0.7450\n",
            "Epoch 656/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.6225 - acc: 0.7998 - val_loss: 0.8065 - val_acc: 0.7492\n",
            "Epoch 657/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5931 - acc: 0.8008 - val_loss: 0.8094 - val_acc: 0.7440\n",
            "Epoch 658/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.5880 - acc: 0.8070 - val_loss: 0.8170 - val_acc: 0.7409\n",
            "Epoch 659/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.5909 - acc: 0.7967 - val_loss: 0.8145 - val_acc: 0.7346\n",
            "Epoch 660/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.6221 - acc: 0.7983 - val_loss: 0.8109 - val_acc: 0.7461\n",
            "Epoch 661/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.5879 - acc: 0.8049 - val_loss: 0.8128 - val_acc: 0.7398\n",
            "Epoch 662/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.5748 - acc: 0.8152 - val_loss: 0.8094 - val_acc: 0.7429\n",
            "Epoch 663/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5936 - acc: 0.8039 - val_loss: 0.8117 - val_acc: 0.7440\n",
            "Epoch 664/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.5909 - acc: 0.8111 - val_loss: 0.8085 - val_acc: 0.7482\n",
            "Epoch 665/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5955 - acc: 0.8091 - val_loss: 0.8063 - val_acc: 0.7440\n",
            "Epoch 666/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5902 - acc: 0.8075 - val_loss: 0.8049 - val_acc: 0.7492\n",
            "Epoch 667/1000\n",
            "1943/1943 [==============================] - 1s 570us/step - loss: 0.5953 - acc: 0.8034 - val_loss: 0.8047 - val_acc: 0.7450\n",
            "Epoch 668/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.6042 - acc: 0.7972 - val_loss: 0.8082 - val_acc: 0.7398\n",
            "Epoch 669/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5922 - acc: 0.8029 - val_loss: 0.8064 - val_acc: 0.7450\n",
            "Epoch 670/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5853 - acc: 0.8024 - val_loss: 0.8072 - val_acc: 0.7335\n",
            "Epoch 671/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.6017 - acc: 0.8209 - val_loss: 0.8060 - val_acc: 0.7482\n",
            "Epoch 672/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5988 - acc: 0.8024 - val_loss: 0.8085 - val_acc: 0.7513\n",
            "Epoch 673/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5929 - acc: 0.8080 - val_loss: 0.8106 - val_acc: 0.7450\n",
            "Epoch 674/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5913 - acc: 0.7983 - val_loss: 0.8044 - val_acc: 0.7440\n",
            "Epoch 675/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5877 - acc: 0.8091 - val_loss: 0.8069 - val_acc: 0.7513\n",
            "Epoch 676/1000\n",
            "1943/1943 [==============================] - 1s 505us/step - loss: 0.6000 - acc: 0.8034 - val_loss: 0.8044 - val_acc: 0.7482\n",
            "Epoch 677/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5934 - acc: 0.8034 - val_loss: 0.8060 - val_acc: 0.7450\n",
            "Epoch 678/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.5830 - acc: 0.8106 - val_loss: 0.8069 - val_acc: 0.7524\n",
            "Epoch 679/1000\n",
            "1943/1943 [==============================] - 1s 566us/step - loss: 0.5924 - acc: 0.8106 - val_loss: 0.8026 - val_acc: 0.7492\n",
            "Epoch 680/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.5902 - acc: 0.8039 - val_loss: 0.8057 - val_acc: 0.7315\n",
            "Epoch 681/1000\n",
            "1943/1943 [==============================] - 1s 583us/step - loss: 0.5864 - acc: 0.8060 - val_loss: 0.8061 - val_acc: 0.7492\n",
            "Epoch 682/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.5992 - acc: 0.7967 - val_loss: 0.8091 - val_acc: 0.7450\n",
            "Epoch 683/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5927 - acc: 0.8034 - val_loss: 0.8081 - val_acc: 0.7503\n",
            "Epoch 684/1000\n",
            "1943/1943 [==============================] - 1s 510us/step - loss: 0.5863 - acc: 0.8168 - val_loss: 0.8035 - val_acc: 0.7524\n",
            "Epoch 685/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5814 - acc: 0.8065 - val_loss: 0.8017 - val_acc: 0.7544\n",
            "Epoch 686/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6014 - acc: 0.7946 - val_loss: 0.8121 - val_acc: 0.7367\n",
            "Epoch 687/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5902 - acc: 0.8111 - val_loss: 0.8049 - val_acc: 0.7555\n",
            "Epoch 688/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.6034 - acc: 0.8024 - val_loss: 0.8047 - val_acc: 0.7513\n",
            "Epoch 689/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5973 - acc: 0.8008 - val_loss: 0.8158 - val_acc: 0.7388\n",
            "Epoch 690/1000\n",
            "1943/1943 [==============================] - 1s 565us/step - loss: 0.6062 - acc: 0.8019 - val_loss: 0.8076 - val_acc: 0.7388\n",
            "Epoch 691/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5777 - acc: 0.8101 - val_loss: 0.8017 - val_acc: 0.7544\n",
            "Epoch 692/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5894 - acc: 0.8044 - val_loss: 0.8058 - val_acc: 0.7409\n",
            "Epoch 693/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.6026 - acc: 0.8060 - val_loss: 0.8081 - val_acc: 0.7409\n",
            "Epoch 694/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5945 - acc: 0.8060 - val_loss: 0.8015 - val_acc: 0.7471\n",
            "Epoch 695/1000\n",
            "1943/1943 [==============================] - 1s 554us/step - loss: 0.5735 - acc: 0.8106 - val_loss: 0.7996 - val_acc: 0.7471\n",
            "Epoch 696/1000\n",
            "1943/1943 [==============================] - 1s 597us/step - loss: 0.5856 - acc: 0.8013 - val_loss: 0.7960 - val_acc: 0.7461\n",
            "Epoch 697/1000\n",
            "1943/1943 [==============================] - 1s 573us/step - loss: 0.5819 - acc: 0.8132 - val_loss: 0.8017 - val_acc: 0.7513\n",
            "Epoch 698/1000\n",
            "1943/1943 [==============================] - 1s 583us/step - loss: 0.5922 - acc: 0.8080 - val_loss: 0.8018 - val_acc: 0.7461\n",
            "Epoch 699/1000\n",
            "1943/1943 [==============================] - 1s 579us/step - loss: 0.5875 - acc: 0.8034 - val_loss: 0.8037 - val_acc: 0.7482\n",
            "Epoch 700/1000\n",
            "1943/1943 [==============================] - 1s 598us/step - loss: 0.5842 - acc: 0.8039 - val_loss: 0.8014 - val_acc: 0.7492\n",
            "Epoch 701/1000\n",
            "1943/1943 [==============================] - 1s 571us/step - loss: 0.5620 - acc: 0.8235 - val_loss: 0.8016 - val_acc: 0.7492\n",
            "Epoch 702/1000\n",
            "1943/1943 [==============================] - 1s 592us/step - loss: 0.5719 - acc: 0.8085 - val_loss: 0.7998 - val_acc: 0.7492\n",
            "Epoch 703/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.5983 - acc: 0.7983 - val_loss: 0.8060 - val_acc: 0.7450\n",
            "Epoch 704/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.5776 - acc: 0.8096 - val_loss: 0.8026 - val_acc: 0.7513\n",
            "Epoch 705/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5885 - acc: 0.7926 - val_loss: 0.7991 - val_acc: 0.7513\n",
            "Epoch 706/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.5908 - acc: 0.8060 - val_loss: 0.7991 - val_acc: 0.7544\n",
            "Epoch 707/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5832 - acc: 0.8157 - val_loss: 0.8022 - val_acc: 0.7471\n",
            "Epoch 708/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.5745 - acc: 0.8214 - val_loss: 0.8039 - val_acc: 0.7544\n",
            "Epoch 709/1000\n",
            "1943/1943 [==============================] - 1s 560us/step - loss: 0.5848 - acc: 0.8065 - val_loss: 0.7958 - val_acc: 0.7544\n",
            "Epoch 710/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5755 - acc: 0.8111 - val_loss: 0.8027 - val_acc: 0.7492\n",
            "Epoch 711/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5722 - acc: 0.8085 - val_loss: 0.7986 - val_acc: 0.7450\n",
            "Epoch 712/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5828 - acc: 0.8065 - val_loss: 0.7944 - val_acc: 0.7513\n",
            "Epoch 713/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5649 - acc: 0.8121 - val_loss: 0.8032 - val_acc: 0.7440\n",
            "Epoch 714/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.5776 - acc: 0.8147 - val_loss: 0.8043 - val_acc: 0.7450\n",
            "Epoch 715/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.5610 - acc: 0.8091 - val_loss: 0.7995 - val_acc: 0.7367\n",
            "Epoch 716/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.5919 - acc: 0.8024 - val_loss: 0.8064 - val_acc: 0.7461\n",
            "Epoch 717/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5743 - acc: 0.8157 - val_loss: 0.8018 - val_acc: 0.7429\n",
            "Epoch 718/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5717 - acc: 0.8199 - val_loss: 0.8023 - val_acc: 0.7367\n",
            "Epoch 719/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5932 - acc: 0.7993 - val_loss: 0.7973 - val_acc: 0.7471\n",
            "Epoch 720/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.5648 - acc: 0.8152 - val_loss: 0.7963 - val_acc: 0.7461\n",
            "Epoch 721/1000\n",
            "1943/1943 [==============================] - 1s 565us/step - loss: 0.5876 - acc: 0.8106 - val_loss: 0.7951 - val_acc: 0.7544\n",
            "Epoch 722/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5573 - acc: 0.8250 - val_loss: 0.7947 - val_acc: 0.7524\n",
            "Epoch 723/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5821 - acc: 0.8085 - val_loss: 0.7945 - val_acc: 0.7534\n",
            "Epoch 724/1000\n",
            "1943/1943 [==============================] - 1s 583us/step - loss: 0.5638 - acc: 0.8075 - val_loss: 0.8010 - val_acc: 0.7356\n",
            "Epoch 725/1000\n",
            "1943/1943 [==============================] - 1s 572us/step - loss: 0.5696 - acc: 0.8163 - val_loss: 0.7999 - val_acc: 0.7461\n",
            "Epoch 726/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5853 - acc: 0.8065 - val_loss: 0.7904 - val_acc: 0.7597\n",
            "Epoch 727/1000\n",
            "1943/1943 [==============================] - 1s 575us/step - loss: 0.5741 - acc: 0.8142 - val_loss: 0.7928 - val_acc: 0.7565\n",
            "Epoch 728/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.5762 - acc: 0.8116 - val_loss: 0.7895 - val_acc: 0.7565\n",
            "Epoch 729/1000\n",
            "1943/1943 [==============================] - 1s 567us/step - loss: 0.5664 - acc: 0.8137 - val_loss: 0.7912 - val_acc: 0.7513\n",
            "Epoch 730/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5877 - acc: 0.8142 - val_loss: 0.7875 - val_acc: 0.7524\n",
            "Epoch 731/1000\n",
            "1943/1943 [==============================] - 1s 565us/step - loss: 0.5737 - acc: 0.8080 - val_loss: 0.7941 - val_acc: 0.7471\n",
            "Epoch 732/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.5790 - acc: 0.8049 - val_loss: 0.7930 - val_acc: 0.7492\n",
            "Epoch 733/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5442 - acc: 0.8245 - val_loss: 0.7908 - val_acc: 0.7555\n",
            "Epoch 734/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.5627 - acc: 0.8157 - val_loss: 0.7908 - val_acc: 0.7565\n",
            "Epoch 735/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5644 - acc: 0.8137 - val_loss: 0.7903 - val_acc: 0.7555\n",
            "Epoch 736/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5796 - acc: 0.8060 - val_loss: 0.7881 - val_acc: 0.7461\n",
            "Epoch 737/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5576 - acc: 0.8183 - val_loss: 0.7915 - val_acc: 0.7503\n",
            "Epoch 738/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.5661 - acc: 0.8188 - val_loss: 0.7870 - val_acc: 0.7492\n",
            "Epoch 739/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5659 - acc: 0.8157 - val_loss: 0.7887 - val_acc: 0.7461\n",
            "Epoch 740/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.5641 - acc: 0.8255 - val_loss: 0.7980 - val_acc: 0.7440\n",
            "Epoch 741/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5760 - acc: 0.8219 - val_loss: 0.8001 - val_acc: 0.7409\n",
            "Epoch 742/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.5707 - acc: 0.8224 - val_loss: 0.7947 - val_acc: 0.7503\n",
            "Epoch 743/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5525 - acc: 0.8157 - val_loss: 0.7927 - val_acc: 0.7576\n",
            "Epoch 744/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5617 - acc: 0.8163 - val_loss: 0.7894 - val_acc: 0.7628\n",
            "Epoch 745/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.5520 - acc: 0.8214 - val_loss: 0.7934 - val_acc: 0.7555\n",
            "Epoch 746/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5683 - acc: 0.8096 - val_loss: 0.7892 - val_acc: 0.7618\n",
            "Epoch 747/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5762 - acc: 0.8085 - val_loss: 0.7875 - val_acc: 0.7524\n",
            "Epoch 748/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5622 - acc: 0.8183 - val_loss: 0.7916 - val_acc: 0.7513\n",
            "Epoch 749/1000\n",
            "1943/1943 [==============================] - 1s 566us/step - loss: 0.5640 - acc: 0.8127 - val_loss: 0.7905 - val_acc: 0.7429\n",
            "Epoch 750/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5637 - acc: 0.8137 - val_loss: 0.7880 - val_acc: 0.7565\n",
            "Epoch 751/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5755 - acc: 0.8116 - val_loss: 0.7920 - val_acc: 0.7513\n",
            "Epoch 752/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5752 - acc: 0.8204 - val_loss: 0.7894 - val_acc: 0.7524\n",
            "Epoch 753/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5540 - acc: 0.8199 - val_loss: 0.7923 - val_acc: 0.7565\n",
            "Epoch 754/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5640 - acc: 0.8096 - val_loss: 0.7846 - val_acc: 0.7471\n",
            "Epoch 755/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5749 - acc: 0.8132 - val_loss: 0.7887 - val_acc: 0.7534\n",
            "Epoch 756/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.5811 - acc: 0.8075 - val_loss: 0.7875 - val_acc: 0.7524\n",
            "Epoch 757/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5850 - acc: 0.8091 - val_loss: 0.7915 - val_acc: 0.7555\n",
            "Epoch 758/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.5625 - acc: 0.8204 - val_loss: 0.7929 - val_acc: 0.7503\n",
            "Epoch 759/1000\n",
            "1943/1943 [==============================] - 1s 582us/step - loss: 0.5639 - acc: 0.8168 - val_loss: 0.7854 - val_acc: 0.7461\n",
            "Epoch 760/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5694 - acc: 0.8271 - val_loss: 0.7874 - val_acc: 0.7555\n",
            "Epoch 761/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5584 - acc: 0.8260 - val_loss: 0.7841 - val_acc: 0.7628\n",
            "Epoch 762/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.5656 - acc: 0.8214 - val_loss: 0.7846 - val_acc: 0.7555\n",
            "Epoch 763/1000\n",
            "1943/1943 [==============================] - 1s 573us/step - loss: 0.5454 - acc: 0.8255 - val_loss: 0.7867 - val_acc: 0.7450\n",
            "Epoch 764/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5760 - acc: 0.8137 - val_loss: 0.7862 - val_acc: 0.7576\n",
            "Epoch 765/1000\n",
            "1943/1943 [==============================] - 1s 569us/step - loss: 0.5600 - acc: 0.8121 - val_loss: 0.7774 - val_acc: 0.7597\n",
            "Epoch 766/1000\n",
            "1943/1943 [==============================] - 1s 577us/step - loss: 0.5617 - acc: 0.8127 - val_loss: 0.7876 - val_acc: 0.7461\n",
            "Epoch 767/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5663 - acc: 0.8163 - val_loss: 0.7803 - val_acc: 0.7628\n",
            "Epoch 768/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5516 - acc: 0.8168 - val_loss: 0.7820 - val_acc: 0.7513\n",
            "Epoch 769/1000\n",
            "1943/1943 [==============================] - 1s 580us/step - loss: 0.5680 - acc: 0.8157 - val_loss: 0.7803 - val_acc: 0.7471\n",
            "Epoch 770/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.5612 - acc: 0.8168 - val_loss: 0.7794 - val_acc: 0.7524\n",
            "Epoch 771/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5541 - acc: 0.8147 - val_loss: 0.7818 - val_acc: 0.7419\n",
            "Epoch 772/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5613 - acc: 0.8065 - val_loss: 0.7846 - val_acc: 0.7628\n",
            "Epoch 773/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5354 - acc: 0.8394 - val_loss: 0.7835 - val_acc: 0.7524\n",
            "Epoch 774/1000\n",
            "1943/1943 [==============================] - 1s 569us/step - loss: 0.5593 - acc: 0.8132 - val_loss: 0.7882 - val_acc: 0.7461\n",
            "Epoch 775/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5678 - acc: 0.8070 - val_loss: 0.7817 - val_acc: 0.7513\n",
            "Epoch 776/1000\n",
            "1943/1943 [==============================] - 1s 557us/step - loss: 0.5534 - acc: 0.8224 - val_loss: 0.7859 - val_acc: 0.7607\n",
            "Epoch 777/1000\n",
            "1943/1943 [==============================] - 1s 492us/step - loss: 0.5548 - acc: 0.8188 - val_loss: 0.7887 - val_acc: 0.7492\n",
            "Epoch 778/1000\n",
            "1943/1943 [==============================] - 1s 583us/step - loss: 0.5620 - acc: 0.8168 - val_loss: 0.7810 - val_acc: 0.7576\n",
            "Epoch 779/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.5585 - acc: 0.8163 - val_loss: 0.7884 - val_acc: 0.7461\n",
            "Epoch 780/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.5683 - acc: 0.8163 - val_loss: 0.7831 - val_acc: 0.7555\n",
            "Epoch 781/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5722 - acc: 0.8132 - val_loss: 0.7884 - val_acc: 0.7513\n",
            "Epoch 782/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5438 - acc: 0.8224 - val_loss: 0.7868 - val_acc: 0.7555\n",
            "Epoch 783/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.5672 - acc: 0.8106 - val_loss: 0.7824 - val_acc: 0.7544\n",
            "Epoch 784/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5859 - acc: 0.8039 - val_loss: 0.7883 - val_acc: 0.7450\n",
            "Epoch 785/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5693 - acc: 0.8044 - val_loss: 0.7849 - val_acc: 0.7492\n",
            "Epoch 786/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.5623 - acc: 0.8173 - val_loss: 0.7797 - val_acc: 0.7534\n",
            "Epoch 787/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5532 - acc: 0.8152 - val_loss: 0.7813 - val_acc: 0.7534\n",
            "Epoch 788/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.5538 - acc: 0.8276 - val_loss: 0.7800 - val_acc: 0.7524\n",
            "Epoch 789/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5690 - acc: 0.8008 - val_loss: 0.7822 - val_acc: 0.7597\n",
            "Epoch 790/1000\n",
            "1943/1943 [==============================] - 1s 582us/step - loss: 0.5627 - acc: 0.8230 - val_loss: 0.7825 - val_acc: 0.7586\n",
            "Epoch 791/1000\n",
            "1943/1943 [==============================] - 1s 567us/step - loss: 0.5441 - acc: 0.8188 - val_loss: 0.7847 - val_acc: 0.7482\n",
            "Epoch 792/1000\n",
            "1943/1943 [==============================] - 1s 553us/step - loss: 0.5388 - acc: 0.8255 - val_loss: 0.7809 - val_acc: 0.7534\n",
            "Epoch 793/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5556 - acc: 0.8230 - val_loss: 0.7757 - val_acc: 0.7555\n",
            "Epoch 794/1000\n",
            "1943/1943 [==============================] - 1s 554us/step - loss: 0.5560 - acc: 0.8116 - val_loss: 0.7773 - val_acc: 0.7607\n",
            "Epoch 795/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.5646 - acc: 0.8168 - val_loss: 0.7758 - val_acc: 0.7503\n",
            "Epoch 796/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.5381 - acc: 0.8250 - val_loss: 0.7751 - val_acc: 0.7597\n",
            "Epoch 797/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5334 - acc: 0.8276 - val_loss: 0.7761 - val_acc: 0.7586\n",
            "Epoch 798/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.5596 - acc: 0.8188 - val_loss: 0.7820 - val_acc: 0.7534\n",
            "Epoch 799/1000\n",
            "1943/1943 [==============================] - 1s 570us/step - loss: 0.5526 - acc: 0.8204 - val_loss: 0.7758 - val_acc: 0.7534\n",
            "Epoch 800/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5517 - acc: 0.8116 - val_loss: 0.7756 - val_acc: 0.7597\n",
            "Epoch 801/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5567 - acc: 0.8157 - val_loss: 0.7744 - val_acc: 0.7607\n",
            "Epoch 802/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5552 - acc: 0.8199 - val_loss: 0.7718 - val_acc: 0.7576\n",
            "Epoch 803/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5568 - acc: 0.8255 - val_loss: 0.7758 - val_acc: 0.7607\n",
            "Epoch 804/1000\n",
            "1943/1943 [==============================] - 1s 555us/step - loss: 0.5254 - acc: 0.8302 - val_loss: 0.7726 - val_acc: 0.7597\n",
            "Epoch 805/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5416 - acc: 0.8178 - val_loss: 0.7751 - val_acc: 0.7607\n",
            "Epoch 806/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5358 - acc: 0.8296 - val_loss: 0.7727 - val_acc: 0.7618\n",
            "Epoch 807/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.5545 - acc: 0.8157 - val_loss: 0.7658 - val_acc: 0.7680\n",
            "Epoch 808/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5502 - acc: 0.8245 - val_loss: 0.7708 - val_acc: 0.7638\n",
            "Epoch 809/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.5515 - acc: 0.8209 - val_loss: 0.7716 - val_acc: 0.7607\n",
            "Epoch 810/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5614 - acc: 0.8085 - val_loss: 0.7720 - val_acc: 0.7607\n",
            "Epoch 811/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.5728 - acc: 0.8121 - val_loss: 0.7734 - val_acc: 0.7618\n",
            "Epoch 812/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.5674 - acc: 0.8101 - val_loss: 0.7719 - val_acc: 0.7638\n",
            "Epoch 813/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.5648 - acc: 0.8194 - val_loss: 0.7798 - val_acc: 0.7565\n",
            "Epoch 814/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5211 - acc: 0.8322 - val_loss: 0.7769 - val_acc: 0.7492\n",
            "Epoch 815/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5407 - acc: 0.8214 - val_loss: 0.7708 - val_acc: 0.7597\n",
            "Epoch 816/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5297 - acc: 0.8255 - val_loss: 0.7788 - val_acc: 0.7544\n",
            "Epoch 817/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5579 - acc: 0.8137 - val_loss: 0.7732 - val_acc: 0.7565\n",
            "Epoch 818/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5582 - acc: 0.8168 - val_loss: 0.7717 - val_acc: 0.7607\n",
            "Epoch 819/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5606 - acc: 0.8101 - val_loss: 0.7728 - val_acc: 0.7544\n",
            "Epoch 820/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5736 - acc: 0.8101 - val_loss: 0.7661 - val_acc: 0.7618\n",
            "Epoch 821/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5408 - acc: 0.8178 - val_loss: 0.7698 - val_acc: 0.7565\n",
            "Epoch 822/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5309 - acc: 0.8266 - val_loss: 0.7710 - val_acc: 0.7586\n",
            "Epoch 823/1000\n",
            "1943/1943 [==============================] - 1s 563us/step - loss: 0.5376 - acc: 0.8286 - val_loss: 0.7694 - val_acc: 0.7576\n",
            "Epoch 824/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5444 - acc: 0.8260 - val_loss: 0.7705 - val_acc: 0.7565\n",
            "Epoch 825/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5548 - acc: 0.8235 - val_loss: 0.7691 - val_acc: 0.7544\n",
            "Epoch 826/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5405 - acc: 0.8214 - val_loss: 0.7709 - val_acc: 0.7524\n",
            "Epoch 827/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5530 - acc: 0.8245 - val_loss: 0.7621 - val_acc: 0.7649\n",
            "Epoch 828/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.5451 - acc: 0.8224 - val_loss: 0.7669 - val_acc: 0.7649\n",
            "Epoch 829/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5420 - acc: 0.8250 - val_loss: 0.7696 - val_acc: 0.7649\n",
            "Epoch 830/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5436 - acc: 0.8250 - val_loss: 0.7753 - val_acc: 0.7670\n",
            "Epoch 831/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5519 - acc: 0.8204 - val_loss: 0.7682 - val_acc: 0.7618\n",
            "Epoch 832/1000\n",
            "1943/1943 [==============================] - 1s 520us/step - loss: 0.5347 - acc: 0.8281 - val_loss: 0.7680 - val_acc: 0.7618\n",
            "Epoch 833/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5227 - acc: 0.8420 - val_loss: 0.7655 - val_acc: 0.7597\n",
            "Epoch 834/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5351 - acc: 0.8157 - val_loss: 0.7609 - val_acc: 0.7565\n",
            "Epoch 835/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5501 - acc: 0.8142 - val_loss: 0.7686 - val_acc: 0.7544\n",
            "Epoch 836/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.5445 - acc: 0.8224 - val_loss: 0.7629 - val_acc: 0.7597\n",
            "Epoch 837/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5571 - acc: 0.8266 - val_loss: 0.7699 - val_acc: 0.7618\n",
            "Epoch 838/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5283 - acc: 0.8322 - val_loss: 0.7635 - val_acc: 0.7670\n",
            "Epoch 839/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5420 - acc: 0.8307 - val_loss: 0.7652 - val_acc: 0.7628\n",
            "Epoch 840/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5390 - acc: 0.8204 - val_loss: 0.7654 - val_acc: 0.7597\n",
            "Epoch 841/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5384 - acc: 0.8286 - val_loss: 0.7640 - val_acc: 0.7607\n",
            "Epoch 842/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.5454 - acc: 0.8245 - val_loss: 0.7653 - val_acc: 0.7659\n",
            "Epoch 843/1000\n",
            "1943/1943 [==============================] - 1s 561us/step - loss: 0.5284 - acc: 0.8286 - val_loss: 0.7667 - val_acc: 0.7659\n",
            "Epoch 844/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5221 - acc: 0.8271 - val_loss: 0.7648 - val_acc: 0.7597\n",
            "Epoch 845/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5250 - acc: 0.8302 - val_loss: 0.7636 - val_acc: 0.7638\n",
            "Epoch 846/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5357 - acc: 0.8266 - val_loss: 0.7646 - val_acc: 0.7670\n",
            "Epoch 847/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5311 - acc: 0.8178 - val_loss: 0.7692 - val_acc: 0.7565\n",
            "Epoch 848/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5475 - acc: 0.8219 - val_loss: 0.7752 - val_acc: 0.7471\n",
            "Epoch 849/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5450 - acc: 0.8188 - val_loss: 0.7738 - val_acc: 0.7471\n",
            "Epoch 850/1000\n",
            "1943/1943 [==============================] - 1s 574us/step - loss: 0.5493 - acc: 0.8230 - val_loss: 0.7649 - val_acc: 0.7586\n",
            "Epoch 851/1000\n",
            "1943/1943 [==============================] - 1s 567us/step - loss: 0.5373 - acc: 0.8271 - val_loss: 0.7676 - val_acc: 0.7586\n",
            "Epoch 852/1000\n",
            "1943/1943 [==============================] - 1s 564us/step - loss: 0.5493 - acc: 0.8209 - val_loss: 0.7646 - val_acc: 0.7524\n",
            "Epoch 853/1000\n",
            "1943/1943 [==============================] - 1s 547us/step - loss: 0.5330 - acc: 0.8296 - val_loss: 0.7641 - val_acc: 0.7607\n",
            "Epoch 854/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5454 - acc: 0.8168 - val_loss: 0.7716 - val_acc: 0.7440\n",
            "Epoch 855/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5457 - acc: 0.8281 - val_loss: 0.7645 - val_acc: 0.7597\n",
            "Epoch 856/1000\n",
            "1943/1943 [==============================] - 1s 539us/step - loss: 0.5410 - acc: 0.8327 - val_loss: 0.7623 - val_acc: 0.7555\n",
            "Epoch 857/1000\n",
            "1943/1943 [==============================] - 1s 530us/step - loss: 0.5412 - acc: 0.8194 - val_loss: 0.7638 - val_acc: 0.7524\n",
            "Epoch 858/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5469 - acc: 0.8219 - val_loss: 0.7623 - val_acc: 0.7618\n",
            "Epoch 859/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5330 - acc: 0.8302 - val_loss: 0.7669 - val_acc: 0.7649\n",
            "Epoch 860/1000\n",
            "1943/1943 [==============================] - 1s 568us/step - loss: 0.5181 - acc: 0.8358 - val_loss: 0.7617 - val_acc: 0.7586\n",
            "Epoch 861/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5364 - acc: 0.8327 - val_loss: 0.7630 - val_acc: 0.7503\n",
            "Epoch 862/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5334 - acc: 0.8281 - val_loss: 0.7638 - val_acc: 0.7586\n",
            "Epoch 863/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5386 - acc: 0.8271 - val_loss: 0.7591 - val_acc: 0.7607\n",
            "Epoch 864/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.5390 - acc: 0.8235 - val_loss: 0.7604 - val_acc: 0.7618\n",
            "Epoch 865/1000\n",
            "1943/1943 [==============================] - 1s 549us/step - loss: 0.5136 - acc: 0.8312 - val_loss: 0.7607 - val_acc: 0.7618\n",
            "Epoch 866/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5233 - acc: 0.8307 - val_loss: 0.7634 - val_acc: 0.7659\n",
            "Epoch 867/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5241 - acc: 0.8291 - val_loss: 0.7569 - val_acc: 0.7659\n",
            "Epoch 868/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.5089 - acc: 0.8430 - val_loss: 0.7589 - val_acc: 0.7565\n",
            "Epoch 869/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.5279 - acc: 0.8178 - val_loss: 0.7563 - val_acc: 0.7565\n",
            "Epoch 870/1000\n",
            "1943/1943 [==============================] - 1s 565us/step - loss: 0.5219 - acc: 0.8281 - val_loss: 0.7580 - val_acc: 0.7586\n",
            "Epoch 871/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.5231 - acc: 0.8312 - val_loss: 0.7599 - val_acc: 0.7618\n",
            "Epoch 872/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5262 - acc: 0.8307 - val_loss: 0.7565 - val_acc: 0.7638\n",
            "Epoch 873/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.5189 - acc: 0.8369 - val_loss: 0.7586 - val_acc: 0.7565\n",
            "Epoch 874/1000\n",
            "1943/1943 [==============================] - 1s 543us/step - loss: 0.5195 - acc: 0.8296 - val_loss: 0.7624 - val_acc: 0.7555\n",
            "Epoch 875/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5313 - acc: 0.8255 - val_loss: 0.7560 - val_acc: 0.7638\n",
            "Epoch 876/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.5280 - acc: 0.8327 - val_loss: 0.7549 - val_acc: 0.7659\n",
            "Epoch 877/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.5256 - acc: 0.8204 - val_loss: 0.7536 - val_acc: 0.7659\n",
            "Epoch 878/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.5417 - acc: 0.8307 - val_loss: 0.7575 - val_acc: 0.7576\n",
            "Epoch 879/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5303 - acc: 0.8348 - val_loss: 0.7574 - val_acc: 0.7597\n",
            "Epoch 880/1000\n",
            "1943/1943 [==============================] - 1s 569us/step - loss: 0.5233 - acc: 0.8271 - val_loss: 0.7562 - val_acc: 0.7555\n",
            "Epoch 881/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5178 - acc: 0.8338 - val_loss: 0.7542 - val_acc: 0.7649\n",
            "Epoch 882/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5194 - acc: 0.8327 - val_loss: 0.7655 - val_acc: 0.7555\n",
            "Epoch 883/1000\n",
            "1943/1943 [==============================] - 1s 532us/step - loss: 0.5237 - acc: 0.8399 - val_loss: 0.7561 - val_acc: 0.7659\n",
            "Epoch 884/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5165 - acc: 0.8358 - val_loss: 0.7619 - val_acc: 0.7618\n",
            "Epoch 885/1000\n",
            "1943/1943 [==============================] - 1s 551us/step - loss: 0.5059 - acc: 0.8415 - val_loss: 0.7679 - val_acc: 0.7555\n",
            "Epoch 886/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5378 - acc: 0.8194 - val_loss: 0.7634 - val_acc: 0.7555\n",
            "Epoch 887/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5375 - acc: 0.8322 - val_loss: 0.7568 - val_acc: 0.7638\n",
            "Epoch 888/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5229 - acc: 0.8302 - val_loss: 0.7541 - val_acc: 0.7722\n",
            "Epoch 889/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.5056 - acc: 0.8399 - val_loss: 0.7590 - val_acc: 0.7555\n",
            "Epoch 890/1000\n",
            "1943/1943 [==============================] - 1s 556us/step - loss: 0.5278 - acc: 0.8327 - val_loss: 0.7660 - val_acc: 0.7597\n",
            "Epoch 891/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.5254 - acc: 0.8281 - val_loss: 0.7523 - val_acc: 0.7649\n",
            "Epoch 892/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.5268 - acc: 0.8291 - val_loss: 0.7514 - val_acc: 0.7638\n",
            "Epoch 893/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5214 - acc: 0.8307 - val_loss: 0.7524 - val_acc: 0.7638\n",
            "Epoch 894/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.5180 - acc: 0.8384 - val_loss: 0.7527 - val_acc: 0.7701\n",
            "Epoch 895/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.5412 - acc: 0.8235 - val_loss: 0.7514 - val_acc: 0.7638\n",
            "Epoch 896/1000\n",
            "1943/1943 [==============================] - 1s 537us/step - loss: 0.5219 - acc: 0.8343 - val_loss: 0.7538 - val_acc: 0.7649\n",
            "Epoch 897/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5067 - acc: 0.8394 - val_loss: 0.7534 - val_acc: 0.7649\n",
            "Epoch 898/1000\n",
            "1943/1943 [==============================] - 1s 521us/step - loss: 0.5414 - acc: 0.8224 - val_loss: 0.7600 - val_acc: 0.7534\n",
            "Epoch 899/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.5024 - acc: 0.8435 - val_loss: 0.7495 - val_acc: 0.7649\n",
            "Epoch 900/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5077 - acc: 0.8343 - val_loss: 0.7618 - val_acc: 0.7638\n",
            "Epoch 901/1000\n",
            "1943/1943 [==============================] - 1s 553us/step - loss: 0.5176 - acc: 0.8230 - val_loss: 0.7496 - val_acc: 0.7712\n",
            "Epoch 902/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.5280 - acc: 0.8343 - val_loss: 0.7582 - val_acc: 0.7680\n",
            "Epoch 903/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.5132 - acc: 0.8332 - val_loss: 0.7533 - val_acc: 0.7628\n",
            "Epoch 904/1000\n",
            "1943/1943 [==============================] - 1s 489us/step - loss: 0.5242 - acc: 0.8379 - val_loss: 0.7521 - val_acc: 0.7607\n",
            "Epoch 905/1000\n",
            "1943/1943 [==============================] - 1s 461us/step - loss: 0.5305 - acc: 0.8240 - val_loss: 0.7557 - val_acc: 0.7628\n",
            "Epoch 906/1000\n",
            "1943/1943 [==============================] - 1s 491us/step - loss: 0.5094 - acc: 0.8405 - val_loss: 0.7508 - val_acc: 0.7597\n",
            "Epoch 907/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.5309 - acc: 0.8307 - val_loss: 0.7491 - val_acc: 0.7659\n",
            "Epoch 908/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.5210 - acc: 0.8255 - val_loss: 0.7534 - val_acc: 0.7597\n",
            "Epoch 909/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.5267 - acc: 0.8312 - val_loss: 0.7500 - val_acc: 0.7607\n",
            "Epoch 910/1000\n",
            "1943/1943 [==============================] - 1s 470us/step - loss: 0.5327 - acc: 0.8296 - val_loss: 0.7516 - val_acc: 0.7680\n",
            "Epoch 911/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.5221 - acc: 0.8312 - val_loss: 0.7520 - val_acc: 0.7680\n",
            "Epoch 912/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.5244 - acc: 0.8157 - val_loss: 0.7553 - val_acc: 0.7544\n",
            "Epoch 913/1000\n",
            "1943/1943 [==============================] - 1s 476us/step - loss: 0.5165 - acc: 0.8327 - val_loss: 0.7458 - val_acc: 0.7638\n",
            "Epoch 914/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.5053 - acc: 0.8441 - val_loss: 0.7516 - val_acc: 0.7555\n",
            "Epoch 915/1000\n",
            "1943/1943 [==============================] - 1s 493us/step - loss: 0.5055 - acc: 0.8430 - val_loss: 0.7435 - val_acc: 0.7670\n",
            "Epoch 916/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.5264 - acc: 0.8214 - val_loss: 0.7458 - val_acc: 0.7680\n",
            "Epoch 917/1000\n",
            "1943/1943 [==============================] - 1s 481us/step - loss: 0.5245 - acc: 0.8255 - val_loss: 0.7488 - val_acc: 0.7597\n",
            "Epoch 918/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5060 - acc: 0.8369 - val_loss: 0.7499 - val_acc: 0.7638\n",
            "Epoch 919/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.5179 - acc: 0.8271 - val_loss: 0.7431 - val_acc: 0.7597\n",
            "Epoch 920/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.5207 - acc: 0.8307 - val_loss: 0.7455 - val_acc: 0.7649\n",
            "Epoch 921/1000\n",
            "1943/1943 [==============================] - 1s 487us/step - loss: 0.5215 - acc: 0.8338 - val_loss: 0.7516 - val_acc: 0.7555\n",
            "Epoch 922/1000\n",
            "1943/1943 [==============================] - 1s 546us/step - loss: 0.5039 - acc: 0.8435 - val_loss: 0.7487 - val_acc: 0.7544\n",
            "Epoch 923/1000\n",
            "1943/1943 [==============================] - 1s 541us/step - loss: 0.5069 - acc: 0.8399 - val_loss: 0.7495 - val_acc: 0.7524\n",
            "Epoch 924/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.5223 - acc: 0.8260 - val_loss: 0.7454 - val_acc: 0.7649\n",
            "Epoch 925/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.5214 - acc: 0.8255 - val_loss: 0.7403 - val_acc: 0.7670\n",
            "Epoch 926/1000\n",
            "1943/1943 [==============================] - 1s 469us/step - loss: 0.5149 - acc: 0.8240 - val_loss: 0.7496 - val_acc: 0.7638\n",
            "Epoch 927/1000\n",
            "1943/1943 [==============================] - 1s 517us/step - loss: 0.5133 - acc: 0.8286 - val_loss: 0.7447 - val_acc: 0.7618\n",
            "Epoch 928/1000\n",
            "1943/1943 [==============================] - 1s 522us/step - loss: 0.5176 - acc: 0.8369 - val_loss: 0.7510 - val_acc: 0.7628\n",
            "Epoch 929/1000\n",
            "1943/1943 [==============================] - 1s 533us/step - loss: 0.5339 - acc: 0.8245 - val_loss: 0.7561 - val_acc: 0.7576\n",
            "Epoch 930/1000\n",
            "1943/1943 [==============================] - 1s 468us/step - loss: 0.5255 - acc: 0.8327 - val_loss: 0.7534 - val_acc: 0.7524\n",
            "Epoch 931/1000\n",
            "1943/1943 [==============================] - 1s 480us/step - loss: 0.5023 - acc: 0.8435 - val_loss: 0.7464 - val_acc: 0.7607\n",
            "Epoch 932/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.4888 - acc: 0.8384 - val_loss: 0.7504 - val_acc: 0.7659\n",
            "Epoch 933/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5151 - acc: 0.8363 - val_loss: 0.7515 - val_acc: 0.7534\n",
            "Epoch 934/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.5247 - acc: 0.8214 - val_loss: 0.7531 - val_acc: 0.7607\n",
            "Epoch 935/1000\n",
            "1943/1943 [==============================] - 1s 490us/step - loss: 0.5052 - acc: 0.8435 - val_loss: 0.7513 - val_acc: 0.7597\n",
            "Epoch 936/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.5173 - acc: 0.8255 - val_loss: 0.7442 - val_acc: 0.7638\n",
            "Epoch 937/1000\n",
            "1943/1943 [==============================] - 1s 478us/step - loss: 0.5049 - acc: 0.8338 - val_loss: 0.7456 - val_acc: 0.7618\n",
            "Epoch 938/1000\n",
            "1943/1943 [==============================] - 1s 475us/step - loss: 0.5219 - acc: 0.8317 - val_loss: 0.7426 - val_acc: 0.7764\n",
            "Epoch 939/1000\n",
            "1943/1943 [==============================] - 1s 486us/step - loss: 0.4939 - acc: 0.8425 - val_loss: 0.7498 - val_acc: 0.7586\n",
            "Epoch 940/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.5118 - acc: 0.8348 - val_loss: 0.7411 - val_acc: 0.7649\n",
            "Epoch 941/1000\n",
            "1943/1943 [==============================] - 1s 500us/step - loss: 0.5259 - acc: 0.8343 - val_loss: 0.7402 - val_acc: 0.7649\n",
            "Epoch 942/1000\n",
            "1943/1943 [==============================] - 1s 487us/step - loss: 0.5117 - acc: 0.8312 - val_loss: 0.7457 - val_acc: 0.7732\n",
            "Epoch 943/1000\n",
            "1943/1943 [==============================] - 1s 490us/step - loss: 0.5112 - acc: 0.8446 - val_loss: 0.7478 - val_acc: 0.7576\n",
            "Epoch 944/1000\n",
            "1943/1943 [==============================] - 1s 492us/step - loss: 0.5190 - acc: 0.8348 - val_loss: 0.7457 - val_acc: 0.7638\n",
            "Epoch 945/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.5176 - acc: 0.8317 - val_loss: 0.7452 - val_acc: 0.7638\n",
            "Epoch 946/1000\n",
            "1943/1943 [==============================] - 1s 483us/step - loss: 0.5150 - acc: 0.8374 - val_loss: 0.7428 - val_acc: 0.7712\n",
            "Epoch 947/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.4954 - acc: 0.8435 - val_loss: 0.7425 - val_acc: 0.7638\n",
            "Epoch 948/1000\n",
            "1943/1943 [==============================] - 1s 481us/step - loss: 0.5130 - acc: 0.8399 - val_loss: 0.7401 - val_acc: 0.7701\n",
            "Epoch 949/1000\n",
            "1943/1943 [==============================] - 1s 529us/step - loss: 0.5216 - acc: 0.8358 - val_loss: 0.7431 - val_acc: 0.7628\n",
            "Epoch 950/1000\n",
            "1943/1943 [==============================] - 1s 479us/step - loss: 0.5269 - acc: 0.8276 - val_loss: 0.7466 - val_acc: 0.7607\n",
            "Epoch 951/1000\n",
            "1943/1943 [==============================] - 1s 552us/step - loss: 0.5007 - acc: 0.8379 - val_loss: 0.7509 - val_acc: 0.7513\n",
            "Epoch 952/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5066 - acc: 0.8322 - val_loss: 0.7518 - val_acc: 0.7618\n",
            "Epoch 953/1000\n",
            "1943/1943 [==============================] - 1s 516us/step - loss: 0.5020 - acc: 0.8456 - val_loss: 0.7449 - val_acc: 0.7628\n",
            "Epoch 954/1000\n",
            "1943/1943 [==============================] - 1s 488us/step - loss: 0.4896 - acc: 0.8446 - val_loss: 0.7407 - val_acc: 0.7628\n",
            "Epoch 955/1000\n",
            "1943/1943 [==============================] - 1s 558us/step - loss: 0.5031 - acc: 0.8405 - val_loss: 0.7405 - val_acc: 0.7712\n",
            "Epoch 956/1000\n",
            "1943/1943 [==============================] - 1s 477us/step - loss: 0.5039 - acc: 0.8348 - val_loss: 0.7386 - val_acc: 0.7618\n",
            "Epoch 957/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5058 - acc: 0.8410 - val_loss: 0.7380 - val_acc: 0.7680\n",
            "Epoch 958/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.4959 - acc: 0.8441 - val_loss: 0.7375 - val_acc: 0.7670\n",
            "Epoch 959/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.4885 - acc: 0.8477 - val_loss: 0.7395 - val_acc: 0.7649\n",
            "Epoch 960/1000\n",
            "1943/1943 [==============================] - 1s 503us/step - loss: 0.5069 - acc: 0.8415 - val_loss: 0.7409 - val_acc: 0.7649\n",
            "Epoch 961/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.5139 - acc: 0.8312 - val_loss: 0.7412 - val_acc: 0.7701\n",
            "Epoch 962/1000\n",
            "1943/1943 [==============================] - 1s 510us/step - loss: 0.4819 - acc: 0.8451 - val_loss: 0.7424 - val_acc: 0.7659\n",
            "Epoch 963/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5046 - acc: 0.8394 - val_loss: 0.7395 - val_acc: 0.7649\n",
            "Epoch 964/1000\n",
            "1943/1943 [==============================] - 1s 527us/step - loss: 0.4956 - acc: 0.8451 - val_loss: 0.7454 - val_acc: 0.7701\n",
            "Epoch 965/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.4994 - acc: 0.8471 - val_loss: 0.7437 - val_acc: 0.7680\n",
            "Epoch 966/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.4938 - acc: 0.8384 - val_loss: 0.7401 - val_acc: 0.7649\n",
            "Epoch 967/1000\n",
            "1943/1943 [==============================] - 1s 513us/step - loss: 0.5144 - acc: 0.8317 - val_loss: 0.7350 - val_acc: 0.7701\n",
            "Epoch 968/1000\n",
            "1943/1943 [==============================] - 1s 514us/step - loss: 0.5053 - acc: 0.8312 - val_loss: 0.7368 - val_acc: 0.7691\n",
            "Epoch 969/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.4849 - acc: 0.8487 - val_loss: 0.7356 - val_acc: 0.7628\n",
            "Epoch 970/1000\n",
            "1943/1943 [==============================] - 1s 534us/step - loss: 0.5021 - acc: 0.8374 - val_loss: 0.7354 - val_acc: 0.7701\n",
            "Epoch 971/1000\n",
            "1943/1943 [==============================] - 1s 519us/step - loss: 0.5009 - acc: 0.8379 - val_loss: 0.7431 - val_acc: 0.7607\n",
            "Epoch 972/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.4986 - acc: 0.8399 - val_loss: 0.7369 - val_acc: 0.7638\n",
            "Epoch 973/1000\n",
            "1943/1943 [==============================] - 1s 538us/step - loss: 0.5126 - acc: 0.8384 - val_loss: 0.7407 - val_acc: 0.7618\n",
            "Epoch 974/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.4976 - acc: 0.8384 - val_loss: 0.7367 - val_acc: 0.7618\n",
            "Epoch 975/1000\n",
            "1943/1943 [==============================] - 1s 507us/step - loss: 0.5000 - acc: 0.8420 - val_loss: 0.7358 - val_acc: 0.7659\n",
            "Epoch 976/1000\n",
            "1943/1943 [==============================] - 1s 573us/step - loss: 0.4981 - acc: 0.8389 - val_loss: 0.7344 - val_acc: 0.7680\n",
            "Epoch 977/1000\n",
            "1943/1943 [==============================] - 1s 550us/step - loss: 0.4920 - acc: 0.8425 - val_loss: 0.7366 - val_acc: 0.7774\n",
            "Epoch 978/1000\n",
            "1943/1943 [==============================] - 1s 540us/step - loss: 0.4991 - acc: 0.8435 - val_loss: 0.7307 - val_acc: 0.7774\n",
            "Epoch 979/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.5179 - acc: 0.8338 - val_loss: 0.7301 - val_acc: 0.7732\n",
            "Epoch 980/1000\n",
            "1943/1943 [==============================] - 1s 545us/step - loss: 0.5034 - acc: 0.8405 - val_loss: 0.7303 - val_acc: 0.7680\n",
            "Epoch 981/1000\n",
            "1943/1943 [==============================] - 1s 542us/step - loss: 0.4963 - acc: 0.8410 - val_loss: 0.7289 - val_acc: 0.7712\n",
            "Epoch 982/1000\n",
            "1943/1943 [==============================] - 1s 518us/step - loss: 0.5008 - acc: 0.8363 - val_loss: 0.7302 - val_acc: 0.7712\n",
            "Epoch 983/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.4924 - acc: 0.8456 - val_loss: 0.7287 - val_acc: 0.7649\n",
            "Epoch 984/1000\n",
            "1943/1943 [==============================] - 1s 472us/step - loss: 0.4930 - acc: 0.8410 - val_loss: 0.7355 - val_acc: 0.7680\n",
            "Epoch 985/1000\n",
            "1943/1943 [==============================] - 1s 548us/step - loss: 0.5033 - acc: 0.8435 - val_loss: 0.7385 - val_acc: 0.7670\n",
            "Epoch 986/1000\n",
            "1943/1943 [==============================] - 1s 485us/step - loss: 0.4791 - acc: 0.8477 - val_loss: 0.7292 - val_acc: 0.7691\n",
            "Epoch 987/1000\n",
            "1943/1943 [==============================] - 1s 531us/step - loss: 0.4915 - acc: 0.8384 - val_loss: 0.7370 - val_acc: 0.7607\n",
            "Epoch 988/1000\n",
            "1943/1943 [==============================] - 1s 535us/step - loss: 0.5111 - acc: 0.8451 - val_loss: 0.7355 - val_acc: 0.7597\n",
            "Epoch 989/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.4996 - acc: 0.8446 - val_loss: 0.7329 - val_acc: 0.7701\n",
            "Epoch 990/1000\n",
            "1943/1943 [==============================] - 1s 524us/step - loss: 0.4930 - acc: 0.8420 - val_loss: 0.7325 - val_acc: 0.7764\n",
            "Epoch 991/1000\n",
            "1943/1943 [==============================] - 1s 544us/step - loss: 0.4906 - acc: 0.8471 - val_loss: 0.7299 - val_acc: 0.7701\n",
            "Epoch 992/1000\n",
            "1943/1943 [==============================] - 1s 525us/step - loss: 0.4974 - acc: 0.8394 - val_loss: 0.7319 - val_acc: 0.7659\n",
            "Epoch 993/1000\n",
            "1943/1943 [==============================] - 1s 523us/step - loss: 0.4840 - acc: 0.8410 - val_loss: 0.7232 - val_acc: 0.7732\n",
            "Epoch 994/1000\n",
            "1943/1943 [==============================] - 1s 508us/step - loss: 0.4907 - acc: 0.8415 - val_loss: 0.7266 - val_acc: 0.7732\n",
            "Epoch 995/1000\n",
            "1943/1943 [==============================] - 1s 528us/step - loss: 0.5009 - acc: 0.8343 - val_loss: 0.7290 - val_acc: 0.7659\n",
            "Epoch 996/1000\n",
            "1943/1943 [==============================] - 1s 526us/step - loss: 0.5095 - acc: 0.8322 - val_loss: 0.7273 - val_acc: 0.7628\n",
            "Epoch 997/1000\n",
            "1943/1943 [==============================] - 1s 587us/step - loss: 0.4882 - acc: 0.8435 - val_loss: 0.7302 - val_acc: 0.7764\n",
            "Epoch 998/1000\n",
            "1943/1943 [==============================] - 1s 559us/step - loss: 0.4866 - acc: 0.8435 - val_loss: 0.7303 - val_acc: 0.7732\n",
            "Epoch 999/1000\n",
            "1943/1943 [==============================] - 1s 509us/step - loss: 0.4799 - acc: 0.8415 - val_loss: 0.7352 - val_acc: 0.7659\n",
            "Epoch 1000/1000\n",
            "1943/1943 [==============================] - 1s 536us/step - loss: 0.5058 - acc: 0.8363 - val_loss: 0.7338 - val_acc: 0.7743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA-90fxWDPhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BsLtP6573ps",
        "colab_type": "code",
        "outputId": "5ce0ef75-0865-44c0-9fc0-7a95896cd422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbJTuu_SDQ_r",
        "colab_type": "code",
        "outputId": "06399091-eb43-426e-d660-ecdf9cf93f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9lT7dKpDUha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(x_testcnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apPHcVHXDoKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snBJBPqgD1lA",
        "colab_type": "code",
        "outputId": "d08da90d-ff1e-49c3-fbe1-532bb57caed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 3, 6, 5, 6, 7, 5, 2, 6, 5, 2, 6, 4, 7, 1, 1, 6, 3, 3, 3, 6, 7,\n",
              "       5, 3, 3, 5, 7, 4, 5, 0, 6, 6, 1, 2, 2, 6, 7, 3, 7, 4, 2, 4, 4, 7,\n",
              "       5, 7, 5, 7, 6, 0, 4, 5, 6, 1, 4, 1, 4, 5, 1, 1, 6, 7, 1, 3, 7, 7,\n",
              "       5, 2, 0, 6, 4, 2, 5, 6, 2, 2, 6, 7, 6, 1, 7, 1, 1, 7, 5, 0, 0, 5,\n",
              "       3, 5, 6, 3, 1, 1, 2, 6, 2, 4, 1, 0, 7, 5, 2, 0, 3, 7, 4, 7, 2, 1,\n",
              "       5, 7, 3, 7, 7, 2, 3, 4, 1, 3, 4, 1, 4, 7, 2, 0, 5, 4, 4, 2, 3, 1,\n",
              "       4, 1, 6, 1, 7, 2, 3, 5, 7, 1, 1, 5, 7, 4, 0, 6, 4, 3, 5, 2, 4, 1,\n",
              "       4, 2, 6, 4, 4, 7, 3, 6, 3, 1, 1, 1, 0, 4, 1, 6, 5, 5, 3, 0, 3, 1,\n",
              "       4, 5, 2, 1, 2, 1, 1, 0, 6, 5, 2, 2, 0, 5, 7, 2, 0, 5, 1, 4, 3, 5,\n",
              "       4, 6, 4, 6, 7, 2, 7, 6, 7, 6, 2, 7, 6, 5, 3, 3, 7, 6, 4, 1, 2, 4,\n",
              "       3, 3, 1, 2, 1, 1, 2, 7, 1, 5, 3, 1, 5, 7, 6, 6, 4, 5, 5, 4, 2, 2,\n",
              "       1, 7, 1, 1, 7, 6, 4, 2, 5, 2, 4, 7, 3, 3, 5, 0, 2, 4, 5, 6, 2, 3,\n",
              "       0, 6, 5, 0, 2, 7, 3, 1, 1, 6, 2, 3, 5, 2, 0, 6, 4, 3, 6, 7, 2, 3,\n",
              "       6, 3, 5, 2, 4, 6, 7, 6, 4, 7, 3, 5, 2, 6, 6, 0, 2, 1, 5, 6, 4, 7,\n",
              "       4, 2, 5, 1, 6, 1, 4, 6, 1, 3, 3, 2, 5, 4, 7, 0, 0, 4, 3, 6, 5, 3,\n",
              "       5, 4, 5, 2, 0, 3, 7, 4, 6, 2, 2, 7, 4, 0, 1, 4, 5, 7, 0, 4, 5, 1,\n",
              "       1, 7, 0, 1, 5, 6, 1, 4, 0, 3, 1, 2, 1, 3, 3, 2, 2, 4, 4, 2, 7, 4,\n",
              "       2, 4, 7, 6, 4, 0, 7, 5, 6, 4, 5, 2, 1, 7, 7, 3, 1, 3, 1, 5, 7, 5,\n",
              "       3, 7, 2, 1, 1, 4, 4, 5, 7, 0, 7, 7, 4, 1, 7, 2, 0, 2, 2, 3, 3, 2,\n",
              "       6, 2, 5, 4, 6, 6, 4, 0, 6, 5, 1, 0, 4, 0, 7, 1, 6, 4, 1, 7, 6, 1,\n",
              "       0, 3, 3, 1, 1, 3, 2, 4, 2, 7, 6, 3, 4, 5, 7, 6, 5, 3, 5, 2, 7, 7,\n",
              "       4, 2, 1, 5, 4, 3, 6, 4, 5, 6, 7, 4, 6, 5, 3, 4, 3, 5, 6, 1, 3, 4,\n",
              "       4, 6, 3, 0, 7, 0, 3, 0, 0, 4, 4, 6, 0, 4, 4, 4, 4, 2, 7, 0, 3, 1,\n",
              "       2, 1, 2, 5, 2, 0, 6, 6, 6, 4, 7, 7, 3, 1, 3, 2, 7, 7, 4, 0, 3, 4,\n",
              "       2, 5, 6, 7, 6, 1, 4, 6, 5, 3, 7, 6, 7, 2, 2, 7, 0, 7, 2, 4, 0, 0,\n",
              "       7, 5, 6, 7, 3, 3, 6, 3, 4, 1, 5, 4, 1, 3, 7, 5, 2, 1, 0, 1, 5, 7,\n",
              "       7, 7, 3, 3, 7, 4, 5, 6, 0, 0, 3, 5, 1, 4, 1, 1, 3, 5, 3, 2, 6, 3,\n",
              "       4, 3, 5, 5, 2, 7, 7, 3, 1, 6, 5, 0, 3, 7, 5, 6, 1, 2, 4, 2, 4, 5,\n",
              "       2, 6, 4, 7, 3, 6, 5, 2, 5, 4, 3, 5, 7, 7, 4, 3, 4, 0, 4, 6, 3, 5,\n",
              "       7, 7, 2, 4, 5, 5, 3, 1, 4, 7, 0, 6, 6, 7, 0, 0, 6, 3, 4, 4, 1, 5,\n",
              "       4, 4, 2, 3, 6, 4, 6, 0, 4, 6, 7, 4, 3, 1, 6, 1, 5, 4, 2, 5, 6, 3,\n",
              "       4, 2, 7, 0, 7, 4, 6, 6, 3, 6, 6, 3, 6, 4, 3, 1, 0, 3, 3, 1, 1, 5,\n",
              "       7, 6, 7, 7, 5, 5, 2, 1, 7, 4, 4, 4, 4, 2, 1, 4, 1, 0, 1, 0, 6, 2,\n",
              "       2, 1, 6, 4, 6, 1, 3, 5, 4, 5, 7, 3, 5, 2, 1, 7, 1, 2, 1, 6, 5, 5,\n",
              "       3, 3, 2, 4, 2, 3, 3, 6, 7, 6, 7, 0, 7, 4, 2, 1, 3, 2, 0, 4, 3, 0,\n",
              "       6, 1, 6, 5, 1, 5, 6, 2, 3, 3, 1, 2, 4, 4, 1, 4, 3, 3, 6, 0, 1, 7,\n",
              "       0, 2, 1, 1, 1, 6, 1, 5, 4, 1, 2, 3, 6, 4, 2, 0, 0, 6, 3, 6, 1, 6,\n",
              "       0, 1, 1, 7, 6, 4, 0, 2, 1, 3, 5, 6, 5, 4, 4, 6, 7, 5, 1, 4, 0, 6,\n",
              "       5, 3, 2, 6, 4, 3, 7, 1, 3, 5, 5, 3, 5, 7, 7, 3, 0, 6, 4, 4, 2, 1,\n",
              "       1, 5, 2, 7, 6, 2, 3, 3, 4, 5, 5, 3, 2, 6, 7, 6, 3, 5, 4, 7, 2, 2,\n",
              "       3, 6, 2, 2, 6, 2, 2, 3, 2, 2, 7, 2, 5, 1, 7, 6, 2, 5, 5, 3, 3, 1,\n",
              "       3, 2, 3, 6, 6, 3, 7, 2, 7, 3, 0, 1, 3, 4, 6, 6, 6, 3, 6, 0, 5, 2,\n",
              "       4, 5, 1, 5, 4, 6, 4, 2, 5, 2, 1, 6, 6, 3, 0, 7, 5, 5, 0, 4, 5, 7,\n",
              "       7, 3, 3, 5, 7, 1, 2, 0, 3, 4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wElwZd6mD3AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIGV2f3tD8dJ",
        "colab_type": "code",
        "outputId": "876d48bd-30a7-4702-cfda-dce82078a2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "new_Ytest"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 3, 6, 5, 6, 7, 5, 2, 6, 5, 2, 6, 4, 7, 1, 1, 6, 3, 3, 3, 6, 7,\n",
              "       5, 3, 3, 5, 7, 4, 5, 0, 6, 6, 1, 2, 2, 6, 7, 3, 7, 4, 2, 4, 4, 7,\n",
              "       5, 7, 5, 7, 6, 0, 4, 5, 6, 1, 4, 1, 4, 5, 1, 1, 6, 7, 1, 3, 7, 7,\n",
              "       5, 2, 0, 6, 4, 2, 5, 6, 2, 2, 6, 7, 6, 1, 7, 1, 1, 7, 5, 0, 0, 5,\n",
              "       3, 5, 6, 3, 1, 1, 2, 6, 2, 4, 1, 0, 7, 5, 2, 0, 3, 7, 4, 7, 2, 1,\n",
              "       5, 7, 3, 7, 7, 2, 3, 4, 1, 3, 4, 1, 4, 7, 2, 0, 5, 4, 4, 2, 3, 1,\n",
              "       4, 1, 6, 1, 7, 2, 3, 5, 7, 1, 1, 5, 7, 4, 0, 6, 4, 3, 5, 2, 4, 1,\n",
              "       4, 2, 6, 4, 4, 7, 3, 6, 3, 1, 1, 1, 0, 4, 1, 6, 5, 5, 3, 0, 3, 1,\n",
              "       4, 5, 2, 1, 2, 1, 1, 0, 6, 5, 2, 2, 0, 5, 7, 2, 0, 5, 1, 4, 3, 5,\n",
              "       4, 6, 4, 6, 7, 2, 7, 6, 7, 6, 2, 7, 6, 5, 3, 3, 7, 6, 4, 1, 2, 4,\n",
              "       3, 3, 1, 2, 1, 1, 2, 7, 1, 5, 3, 1, 5, 7, 6, 6, 4, 5, 5, 4, 2, 2,\n",
              "       1, 7, 1, 1, 7, 6, 4, 2, 5, 2, 4, 7, 3, 3, 5, 0, 2, 4, 5, 6, 2, 3,\n",
              "       0, 6, 5, 0, 2, 7, 3, 1, 1, 6, 2, 3, 5, 2, 0, 6, 4, 3, 6, 7, 2, 3,\n",
              "       6, 3, 5, 2, 4, 6, 7, 6, 4, 7, 3, 5, 2, 6, 6, 0, 2, 1, 5, 6, 4, 7,\n",
              "       4, 2, 5, 1, 6, 1, 4, 6, 1, 3, 3, 2, 5, 4, 7, 0, 0, 4, 3, 6, 5, 3,\n",
              "       5, 4, 5, 2, 0, 3, 7, 4, 6, 2, 2, 7, 4, 0, 1, 4, 5, 7, 0, 4, 5, 1,\n",
              "       1, 7, 0, 1, 5, 6, 1, 4, 0, 3, 1, 2, 1, 3, 3, 2, 2, 4, 4, 2, 7, 4,\n",
              "       2, 4, 7, 6, 4, 0, 7, 5, 6, 4, 5, 2, 1, 7, 7, 3, 1, 3, 1, 5, 7, 5,\n",
              "       3, 7, 2, 1, 1, 4, 4, 5, 7, 0, 7, 7, 4, 1, 7, 2, 0, 2, 2, 3, 3, 2,\n",
              "       6, 2, 5, 4, 6, 6, 4, 0, 6, 5, 1, 0, 4, 0, 7, 1, 6, 4, 1, 7, 6, 1,\n",
              "       0, 3, 3, 1, 1, 3, 2, 4, 2, 7, 6, 3, 4, 5, 7, 6, 5, 3, 5, 2, 7, 7,\n",
              "       4, 2, 1, 5, 4, 3, 6, 4, 5, 6, 7, 4, 6, 5, 3, 4, 3, 5, 6, 1, 3, 4,\n",
              "       4, 6, 3, 0, 7, 0, 3, 0, 0, 4, 4, 6, 0, 4, 4, 4, 4, 2, 7, 0, 3, 1,\n",
              "       2, 1, 2, 5, 2, 0, 6, 6, 6, 4, 7, 7, 3, 1, 3, 2, 7, 7, 4, 0, 3, 4,\n",
              "       2, 5, 6, 7, 6, 1, 4, 6, 5, 3, 7, 6, 7, 2, 2, 7, 0, 7, 2, 4, 0, 0,\n",
              "       7, 5, 6, 7, 3, 3, 6, 3, 4, 1, 5, 4, 1, 3, 7, 5, 2, 1, 0, 1, 5, 7,\n",
              "       7, 7, 3, 3, 7, 4, 5, 6, 0, 0, 3, 5, 1, 4, 1, 1, 3, 5, 3, 2, 6, 3,\n",
              "       4, 3, 5, 5, 2, 7, 7, 3, 1, 6, 5, 0, 3, 7, 5, 6, 1, 2, 4, 2, 4, 5,\n",
              "       2, 6, 4, 7, 3, 6, 5, 2, 5, 4, 3, 5, 7, 7, 4, 3, 4, 0, 4, 6, 3, 5,\n",
              "       7, 7, 2, 4, 5, 5, 3, 1, 4, 7, 0, 6, 6, 7, 0, 0, 6, 3, 4, 4, 1, 5,\n",
              "       4, 4, 2, 3, 6, 4, 6, 0, 4, 6, 7, 4, 3, 1, 6, 1, 5, 4, 2, 5, 6, 3,\n",
              "       4, 2, 7, 0, 7, 4, 6, 6, 3, 6, 6, 3, 6, 4, 3, 1, 0, 3, 3, 1, 1, 5,\n",
              "       7, 6, 7, 7, 5, 5, 2, 1, 7, 4, 4, 4, 4, 2, 1, 4, 1, 0, 1, 0, 6, 2,\n",
              "       2, 1, 6, 4, 6, 1, 3, 5, 4, 5, 7, 3, 5, 2, 1, 7, 1, 2, 1, 6, 5, 5,\n",
              "       3, 3, 2, 4, 2, 3, 3, 6, 7, 6, 7, 0, 7, 4, 2, 1, 3, 2, 0, 4, 3, 0,\n",
              "       6, 1, 6, 5, 1, 5, 6, 2, 3, 3, 1, 2, 4, 4, 1, 4, 3, 3, 6, 0, 1, 7,\n",
              "       0, 2, 1, 1, 1, 6, 1, 5, 4, 1, 2, 3, 6, 4, 2, 0, 0, 6, 3, 6, 1, 6,\n",
              "       0, 1, 1, 7, 6, 4, 0, 2, 1, 3, 5, 6, 5, 4, 4, 6, 7, 5, 1, 4, 0, 6,\n",
              "       5, 3, 2, 6, 4, 3, 7, 1, 3, 5, 5, 3, 5, 7, 7, 3, 0, 6, 4, 4, 2, 1,\n",
              "       1, 5, 2, 7, 6, 2, 3, 3, 4, 5, 5, 3, 2, 6, 7, 6, 3, 5, 4, 7, 2, 2,\n",
              "       3, 6, 2, 2, 6, 2, 2, 3, 2, 2, 7, 2, 5, 1, 7, 6, 2, 5, 5, 3, 3, 1,\n",
              "       3, 2, 3, 6, 6, 3, 7, 2, 7, 3, 0, 1, 3, 4, 6, 6, 6, 3, 6, 0, 5, 2,\n",
              "       4, 5, 1, 5, 4, 6, 4, 2, 5, 2, 1, 6, 6, 3, 0, 7, 5, 5, 0, 4, 5, 7,\n",
              "       7, 3, 3, 5, 7, 1, 2, 0, 3, 4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gncsFSHmEANB",
        "colab_type": "code",
        "outputId": "470a861c-02d2-4e3b-e4a8-89b0a6b73ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.51      0.62        73\n",
            "           1       0.79      0.95      0.86       122\n",
            "           2       0.77      0.64      0.70       121\n",
            "           3       0.68      0.69      0.68       130\n",
            "           4       0.81      0.78      0.80       137\n",
            "           5       0.80      0.84      0.82       120\n",
            "           6       0.76      0.85      0.81       131\n",
            "           7       0.81      0.81      0.81       123\n",
            "\n",
            "    accuracy                           0.77       957\n",
            "   macro avg       0.78      0.76      0.76       957\n",
            "weighted avg       0.78      0.77      0.77       957\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXGcLUh-ENGt",
        "colab_type": "code",
        "outputId": "44b63b91-5305-4c99-e945-b74d2d6a72c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 37   9   4  17   0   4   1   1]\n",
            " [  1 116   1   1   0   0   3   0]\n",
            " [  0   5  78   9  10   7   4   8]\n",
            " [  3  13   3  90   4  10   2   5]\n",
            " [  0   1   6   2 107   3  16   2]\n",
            " [  0   3   3   5   4 101   4   0]\n",
            " [  1   0   0   6   2   2 112   8]\n",
            " [  4   0   6   3   5   0   5 100]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X8XpPz4ET7i",
        "colab_type": "text"
      },
      "source": [
        "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIA8Vf8GEaSS",
        "colab_type": "text"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_drVHzKEN5d",
        "colab_type": "code",
        "outputId": "72e37a3d-5e94-428c-b3d6-84ff5868ff44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ymsLDTgEqOM",
        "colab_type": "text"
      },
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMwbX3r3Euby",
        "colab_type": "code",
        "outputId": "71232edb-740a-4813-8476-f79914e8c351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUa8w1a-FFlZ",
        "colab_type": "text"
      },
      "source": [
        "# Checking the accuracy of the loaded model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6_J6trRFS-y",
        "colab_type": "code",
        "outputId": "2a32c9c2-9aaf-474b-adc4-1ace34ffc719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "957/957 [==============================] - 0s 194us/step\n",
            "Restored model, accuracy: 77.43%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}